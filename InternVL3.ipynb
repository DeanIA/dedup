{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fd6c74",
   "metadata": {},
   "source": [
    "## Import & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55fa6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/jupyter-dai7591/.cache/huggingface/modules/transformers_modules/OpenGVLab/ViCLIP-L-14-hf/1652361522e1cb41c28cdfae870f690d00e7456b/viclip_text.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import av\n",
    "import cv2\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import faiss\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "VIDEO_DIR       = \"input_videos\"\n",
    "OUTPUT_DIR      = \"videoclip_output\"\n",
    "FRAMES_PER_CLIP = 8\n",
    "CLIP_DURATION   = 30\n",
    "BATCH_SIZE      = 128\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    device = \"cuda:0\"\n",
    "    print(f\"device:{device}\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"device:{device}\")\n",
    "else:\n",
    "    print(f\"Plain ol' CPU\")\n",
    "\n",
    "# ==== LOAD ViCLIP ====\n",
    "model = AutoModel.from_pretrained(\"OpenGVLab/ViCLIP-L-14-hf\", trust_remote_code=True).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccda9d",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776706a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL-14B-224px:\n",
      "- configuration_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL-14B-224px:\n",
      "- configuration_internvl.py\n",
      "- configuration_intern_vit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Encountered exception while importing peft: No module named 'peft'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: peft. Run `pip install peft`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ─── 1) Load the model & image processor ───\u001b[39;00m\n\u001b[1;32m     10\u001b[0m MODEL_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenGVLab/InternVL-14B-224px\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     18\u001b[0m image_processor \u001b[38;5;241m=\u001b[39m CLIPImageProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_ID)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ─── 2) Helper to sample & preprocess frames ───\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:558\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m    557\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:558\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m     code_revision \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m final_module \u001b[38;5;241m=\u001b[39m \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload\u001b[38;5;241m=\u001b[39mforce_download)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:383\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[0;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m modules_needed \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m full_submodule \u001b[38;5;241m=\u001b[39m TRANSFORMERS_DYNAMIC_MODULE_NAME \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m submodule\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:215\u001b[0m, in \u001b[0;36mcheck_imports\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
      "\u001b[0;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: peft. Run `pip install peft`"
     ]
    }
   ],
   "source": [
    "# extract_embeddings.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from decord import VideoReader, cpu\n",
    "from transformers import AutoModel, CLIPImageProcessor\n",
    "\n",
    "# ─── 1) Load the model & image processor ───\n",
    "MODEL_ID = \"OpenGVLab/InternVL-14B-224px\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "image_processor = CLIPImageProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# ─── 2) Helper to sample & preprocess frames ───\n",
    "def extract_frames(video_path, num_frames=32):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    total = len(vr)\n",
    "    interval = math.ceil(total / num_frames)\n",
    "    # pick center of each interval\n",
    "    idxs = (np.arange(0, total, interval) + interval // 2).clip(max=total-1)\n",
    "    frames = vr.get_batch(idxs).asnumpy()  # shape: [num_frames, H, W, 3]\n",
    "    return frames\n",
    "\n",
    "# ─── 3) Main embedding extraction ───\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"video_path\", type=str, help=\"Path to input video file\")\n",
    "    parser.add_argument(\"--frames\", type=int, default=32, help=\"How many frames to sample\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 3.1 Sample raw frames\n",
    "    raw_frames = extract_frames(args.video_path, num_frames=args.frames)\n",
    "\n",
    "    # 3.2 Turn into model inputs\n",
    "    # image_processor will resize, normalize, and batchify\n",
    "    batch = image_processor(images=list(raw_frames), return_tensors=\"pt\")\n",
    "    pixel_values = batch.pixel_values.to(torch.bfloat16).cuda()  # shape [B, 3, H, W]\n",
    "\n",
    "    # 3.3 Encode & pool\n",
    "    with torch.no_grad():\n",
    "        feats = model.encode_image(pixel_values, mode=\"InternVL-C\")  # [B, D]\n",
    "        # average pooling → one vector per video\n",
    "        video_embed = feats.mean(dim=0)                           # [D]\n",
    "        video_embed = torch.nn.functional.normalize(video_embed, dim=-1)\n",
    "\n",
    "    # 3.4 Detach to NumPy\n",
    "    emb_np = video_embed.cpu().numpy()\n",
    "    print(f\"Extracted video embedding (shape {emb_np.shape}):\")\n",
    "    print(emb_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5094e",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab54de12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Run:\n",
      "  tensorboard --logdir=runs/embeds\n",
      "Then open http://localhost:6006/#projector\n"
     ]
    }
   ],
   "source": [
    "# Load lookup metadata\n",
    "with open(\"videoclip_output/embedding_lookup.json\", \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Ensure entries are sorted by clip_id (assumed to match row index)\n",
    "sorted_lookup = sorted(lookup.values(), key=lambda x: int(x[\"clip_id\"]))\n",
    "\n",
    "# Create metadata strings like: \"video.mp4[3] @ 90s\"\n",
    "metadata = [\n",
    "    f'{entry[\"video_file\"]}[{entry[\"clip_index\"]}] @ {entry[\"start_time_sec\"]}s'\n",
    "    for entry in sorted_lookup\n",
    "]\n",
    "\n",
    "# Load embedding matrix\n",
    "emb_matrix = np.load(\"videoclip_output/video_embeddings.npy\")\n",
    "\n",
    "# Ensure alignment\n",
    "assert emb_matrix.shape[0] == len(metadata), (\n",
    "    f\"❌ {emb_matrix.shape[0]} embeddings vs {len(metadata)} metadata entries\"\n",
    ")\n",
    "\n",
    "# Write to TensorBoard\n",
    "writer = SummaryWriter(log_dir=\"runs/embeds\")\n",
    "writer.add_embedding(\n",
    "    emb_matrix,\n",
    "    metadata=metadata,\n",
    "    tag=\"my_embeddings\"\n",
    ")\n",
    "writer.close()\n",
    "\n",
    "print(\"✅ Done. Run:\\n  tensorboard --logdir=runs/embeds\\nThen open http://localhost:6006/#projector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c43c8",
   "metadata": {},
   "source": [
    "## Index in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28beaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH    = \"videoclip_output/video_embeddings.npy\"\n",
    "LOOKUP_PATH = \"videoclip_output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"videoclip_output/video_embeddings.index\"\n",
    "\n",
    "# Load embeddings and lookup dict\n",
    "embeddings = np.load(EMB_PATH)  \n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Build FAISS index (inner-product) and add IDs\n",
    "faiss.normalize_L2(embeddings)\n",
    "dim   = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "ids = np.arange(embeddings.shape[0], dtype=\"int64\")\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "# Save the index for later\n",
    "faiss.write_index(index, INDEX_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4f644",
   "metadata": {},
   "source": [
    "## Find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e27c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNS_0030_V.mp4 <--> TNS_0031_V.mp4 | similar clips: 205 | avg distance: 1.0000\n",
      "TNS_0024_V.mp4 <--> TNS_0025_V.mp4 | similar clips: 59 | avg distance: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "def count_similar_clip_pairs(radius: float):\n",
    "    \"\"\"\n",
    "    For each unique pair of video files, count how many clip pairs are similar\n",
    "    (i.e., within the given radius), and record the average distance.\n",
    "    Returns: list of (file1, file2, count, avg_distance)\n",
    "    \"\"\"\n",
    "    embeddings = np.load(EMB_PATH)\n",
    "    with open(LOOKUP_PATH, \"r\") as f:\n",
    "        lookup = json.load(f)\n",
    "\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    lims, distances, labels = index.range_search(embeddings, radius)\n",
    "\n",
    "    # Map (file1, file2) to list of distances\n",
    "    pair_stats = defaultdict(list)\n",
    "\n",
    "    for q in range(len(embeddings)):\n",
    "        start, end = lims[q], lims[q+1]\n",
    "        for i in range(start, end):\n",
    "            idx = labels[i]\n",
    "            if idx <= q:\n",
    "                continue  # skip self or repeated\n",
    "            f1 = lookup[str(q)][\"video_file\"]\n",
    "            f2 = lookup[str(idx)][\"video_file\"]\n",
    "            if f1 != f2:\n",
    "                key = tuple(sorted((f1, f2)))\n",
    "                pair_stats[key].append(distances[i])\n",
    "\n",
    "    results = []\n",
    "    for (f1, f2), dists in pair_stats.items():\n",
    "        avg_dist = sum(dists) / len(dists)\n",
    "        results.append((f1, f2, len(dists), avg_dist))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Choose your similarity radius (e.g., 0.8 for cosine similarity >= 0.8)\n",
    "radius = 0.999\n",
    "pairs = count_similar_clip_pairs(radius)\n",
    "\n",
    "for f1, f2, count, avg_dist in sorted(pairs, key=lambda x: -x[2]):\n",
    "    print(f\"{f1} <--> {f2} | similar clips: {count} | avg distance: {avg_dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf210fa",
   "metadata": {},
   "source": [
    "## Evaluate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61aef684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ True Positives (correctly detected): 2\n",
      "❌ False Positives (wrongly flagged):   0\n",
      "🔍 False Negatives (missed duplicates): 4685308\n",
      "📈 Recall: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "with open(\"dup_groundtruth.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    # if your CSV has a header, skip it:\n",
    "    next(reader, None)\n",
    "    # columns: filename, status\n",
    "    ground_truth = {row[0]: row[1] for row in reader}\n",
    "\n",
    "\n",
    "def evaluate_duplicates(pairs, ground_truth):\n",
    "    def extract_id(filename):\n",
    "        match = re.search(r'TNS_(\\d+)', filename)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    # Normalize detected pairs based on ID number\n",
    "    detected_duplicates = set()\n",
    "    for f1, f2, _, _ in pairs:\n",
    "        id1 = extract_id(f1)\n",
    "        id2 = extract_id(f2)\n",
    "        if id1 and id2:\n",
    "            detected_duplicates.add(tuple(sorted([id1, id2])))\n",
    "\n",
    "    # Normalize ground-truth: find all DUPLICATE → PRINCIPAL mappings\n",
    "    true_duplicates = {k for k, v in ground_truth.items() if v.upper() == \"DUPLICATE\"}\n",
    "    true_pairs = set()\n",
    "    for dup in true_duplicates:\n",
    "        dup_id = extract_id(dup)\n",
    "        for principal, status in ground_truth.items():\n",
    "            if status.upper() == \"PRINCIPAL\":\n",
    "                principal_id = extract_id(principal)\n",
    "                if dup_id and principal_id:\n",
    "                    true_pairs.add(tuple(sorted([dup_id, principal_id])))\n",
    "\n",
    "    # Evaluation\n",
    "    true_positives = detected_duplicates & true_pairs\n",
    "    false_positives = detected_duplicates - true_pairs\n",
    "    false_negatives = true_pairs - detected_duplicates\n",
    "\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if (len(true_positives) + len(false_negatives)) > 0 else 0.0\n",
    "\n",
    "    print(f\"✅ True Positives (correctly detected): {len(true_positives)}\")\n",
    "    print(f\"❌ False Positives (wrongly flagged):   {len(false_positives)}\")\n",
    "    print(f\"🔍 False Negatives (missed duplicates): {len(false_negatives)}\")\n",
    "    print(f\"📈 Recall: {recall:.2%}\")\n",
    "\n",
    "    return {\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "eval_result = evaluate_duplicates(pairs, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14c7f8",
   "metadata": {},
   "source": [
    "## Find duplicates and optimal radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85b630e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   radius  true_positives  false_positives  false_negatives        recall\n",
      "0   0.900               3               28          4685307  6.402991e-07\n",
      "1   0.950               2               10          4685308  4.268661e-07\n",
      "2   0.990               2                0          4685308  4.268661e-07\n",
      "3   0.999               2                0          4685308  4.268661e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to embeddings, lookup, and index\n",
    "EMB_PATH    = 'videoclip_output/video_embeddings.npy'\n",
    "LOOKUP_PATH = 'videoclip_output/embedding_lookup.json'\n",
    "INDEX_PATH  = 'videoclip_output/video_embeddings.index'\n",
    "\n",
    "def count_similar_clip_pairs(radius):\n",
    "    embeddings = np.load(EMB_PATH)\n",
    "    with open(LOOKUP_PATH) as f:\n",
    "        lookup = json.load(f)\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    lims, distances, labels = index.range_search(embeddings, radius)\n",
    "\n",
    "    pair_stats = defaultdict(list)\n",
    "    for q in range(len(embeddings)):\n",
    "        for i in range(lims[q], lims[q+1]):\n",
    "            idx = labels[i]\n",
    "            if idx <= q:\n",
    "                continue\n",
    "            f1 = lookup[str(q)]['video_file']\n",
    "            f2 = lookup[str(idx)]['video_file']\n",
    "            if f1 != f2:\n",
    "                pair = tuple(sorted((f1, f2)))\n",
    "                pair_stats[pair].append(distances[i])\n",
    "\n",
    "    return [\n",
    "        (f1, f2, len(dists), sum(dists)/len(dists))\n",
    "        for (f1, f2), dists in pair_stats.items()\n",
    "    ]\n",
    "\n",
    "def evaluate_duplicates(pairs, ground_truth):\n",
    "    def extract_id(name):\n",
    "        m = re.search(r'TNS_(\\d+)', name)\n",
    "        return m.group(1) if m else None\n",
    "\n",
    "    detected = {\n",
    "        tuple(sorted((extract_id(f1), extract_id(f2))))\n",
    "        for f1, f2, _, _ in pairs\n",
    "    }\n",
    "    truth_dups = {\n",
    "        extract_id(k)\n",
    "        for k, v in ground_truth.items()\n",
    "        if v.upper() == 'DUPLICATE'\n",
    "    }\n",
    "    truth_pairs = {\n",
    "        tuple(sorted((dup, extract_id(pr))))\n",
    "        for dup in truth_dups\n",
    "        for pr, st in ground_truth.items()\n",
    "        if st.upper() == 'PRINCIPAL'\n",
    "    }\n",
    "\n",
    "    tp = len(detected & truth_pairs)\n",
    "    fp = len(detected - truth_pairs)\n",
    "    fn = len(truth_pairs - detected)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# --- Load ground truth ---\n",
    "gt_df = pd.read_csv('dup_groundtruth.csv')  # comma-separated file\n",
    "ground_truth = dict(zip(gt_df['UAR Code'], gt_df['Principal / Duplicate']))\n",
    "\n",
    "# --- Loop over different radii ---\n",
    "radii = [0.9, 0.95, 0.99, 0.999]\n",
    "results = []\n",
    "\n",
    "for r in radii:\n",
    "    pairs   = count_similar_clip_pairs(r)\n",
    "    metrics = evaluate_duplicates(pairs, ground_truth)\n",
    "    results.append({'radius': r, **metrics})\n",
    "\n",
    "# --- Export & display ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('evaluation_results.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09882c39",
   "metadata": {},
   "source": [
    "## Search prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "LOOKUP_PATH = \"output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"output/video_embeddings.index\"\n",
    "MODEL_CHECKPOINT = \"microsoft/xclip-base-patch32\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Load lookup table and FAISS index\n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "# Load tokenizer & text model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModel.from_pretrained(MODEL_CHECKPOINT).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def search_prompts(prompts, top_k=1):\n",
    "    \"\"\"\n",
    "    Encode text prompts, search the FAISS index, and return\n",
    "    filename + timestamp for each top-k match.\n",
    "    \"\"\"\n",
    "    # Tokenize and encode\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        text_feats = model.get_text_features(**inputs)\n",
    "    text_feats = text_feats.cpu().numpy()\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(text_feats)\n",
    "\n",
    "    # Search\n",
    "    D, I = index.search(text_feats, top_k)\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        matches = []\n",
    "        for score, clip_id in zip(D[i], I[i]):\n",
    "            info = lookup[str(int(clip_id))]\n",
    "            matches.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"file\": info[\"video_file\"],\n",
    "                \"start_time_sec\": info[\"start_time_sec\"],\n",
    "                \"clip_index\": info[\"clip_index\"],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        results.append(matches)\n",
    "    return results\n",
    "\n",
    "prompts = [\n",
    "    \"Videos of a man injured in the daytime. Smoke is rising in the background\",\n",
    "    \"A clown eating a huge bowl of spagetti while riding a bicycle\"\n",
    "]\n",
    "results = search_prompts(prompts, top_k=3)\n",
    "for match_list in results:\n",
    "    for match in match_list:\n",
    "        print(f\"Prompt: {match['prompt']}\")\n",
    "        print(f\"  File: {match['file']}\")\n",
    "        print(f\"  Start time: {match['start_time_sec']}s (clip index {match['clip_index']})\")\n",
    "        print(f\"  Similarity: {match['similarity']:.4f}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dedup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
