{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fd6c74",
   "metadata": {},
   "source": [
    "## Import & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa6df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "VIDEO_DIR        = \"input_videos\"  # set your directory path here\n",
    "OUTPUT_DIR       = \"output\"\n",
    "FRAMES_PER_CLIP  = 8        # frames per clip\n",
    "FRAME_SAMPLE     = 1        # sample every frame (sampling rate multiplier)\n",
    "BATCH_SIZE       = 16      # clips per model inference batch\n",
    "MODEL_CHECKPOINT = \"alibaba-pai/VideoCLIP-XL-v2\"\n",
    "CLIP_DURATION    = 10       # Duration of each window in seconds\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    DEVICE = \"cuda:1\"\n",
    "    print(f\"device:{DEVICE}\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(f\"device:{DEVICE}\")\n",
    "else:\n",
    "    print(f\"Plain ol' CPU\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Prepare output\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load processor & model\n",
    "device = torch.device(DEVICE)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_CHECKPOINT, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = model.tokenizer\n",
    "models = {\"viclip\": model, \"tokenizer\": tokenizer}\n",
    "print(\"loaded VideoCLIP-XL-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccda9d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178686fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_clip(container, indices, fps):\n",
    "    \"\"\"\n",
    "    For each requested frame index, seek directly to its timestamp\n",
    "    and decode just one frame.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        # convert frame index → µs timestamp\n",
    "        ts = int((idx / fps) * 1e6)\n",
    "        # seek to the nearest keyframe before ts\n",
    "        container.seek(ts, any_frame=False, backward=True)\n",
    "        # decode until we get one video frame, then stop\n",
    "        for frame in container.decode(video=0):\n",
    "            frames.append(frame.to_ndarray(format=\"rgb24\"))\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "\n",
    "def process_directory_to_embeddings(\n",
    "    video_dir: str,\n",
    "    clip_len: int = FRAMES_PER_CLIP,\n",
    "    frame_sample_rate: int = FRAME_SAMPLE,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    output_dir: str = OUTPUT_DIR,\n",
    "):\n",
    "    \"\"\"\n",
    "    Walk through all video files in `video_dir`, extract fixed-length clips every 10 seconds,\n",
    "    batch them, encode with XCLIP, and save embeddings + lookup metadata.\n",
    "    \"\"\"\n",
    "    embedding_batches = []\n",
    "    lookup_dict       = {}\n",
    "    global_clip_id    = 0\n",
    "    current_batch     = []\n",
    "    current_meta      = []\n",
    "\n",
    "    # Supported video extensions\n",
    "    exts = ('.mp4', '.mov', '.avi')\n",
    "\n",
    "    for filename in sorted(os.listdir(video_dir)):\n",
    "        if not filename.lower().endswith(exts):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_dir, filename)\n",
    "        container  = av.open(video_path)\n",
    "        stream     = container.streams.video[0]\n",
    "        total_frames = stream.frames\n",
    "        fps = float(stream.average_rate) if stream.average_rate else 1.0\n",
    "        \n",
    "        clip_index = 0 \n",
    "        # Define a 10-second window in frames\n",
    "        window_size = int(10 * fps)\n",
    "        for start in range(0, total_frames - window_size + 1, window_size):\n",
    "            indices = np.linspace(start,\n",
    "                                start + window_size,\n",
    "                                num=clip_len,\n",
    "                                endpoint=False,\n",
    "                                dtype=np.int64)\n",
    "            frames = read_video_clip(container, indices, fps)\n",
    "            if len(frames) < clip_len:\n",
    "                continue\n",
    "\n",
    "            current_batch.append(frames)\n",
    "            current_meta.append({\n",
    "                \"clip_id\":        global_clip_id,\n",
    "                \"clip_index\":     clip_index,\n",
    "                \"video_file\":     filename,\n",
    "                \"start_time_sec\": int(start / fps),\n",
    "            })\n",
    "\n",
    "            clip_index     += 1\n",
    "            global_clip_id += 1\n",
    "\n",
    "            # When batch is ready, run inference\n",
    "            if len(current_batch) == batch_size:\n",
    "                first_id = current_meta[0][\"clip_id\"]\n",
    "                last_id  = current_meta[-1][\"clip_id\"]\n",
    "                print(f\"Running inference on batch of {batch_size} clips: IDs {first_id}-{last_id}\")\n",
    "\n",
    "                inputs = processor(videos=current_batch, return_tensors=\"pt\", padding=True)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    embeds = model.get_video_features(**inputs).cpu().numpy()\n",
    "\n",
    "                embedding_batches.append(embeds)\n",
    "                for m in current_meta:\n",
    "                    lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "                current_batch = []\n",
    "                current_meta  = []\n",
    "\n",
    "        container.close()\n",
    "\n",
    "    # Process any remaining clips\n",
    "    if current_batch:\n",
    "        print(f\"Running inference on final batch of {len(current_batch)} clips: IDs {current_meta[0]['clip_id']}-{current_meta[-1]['clip_id']}\")\n",
    "        inputs = processor(videos=current_batch, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            embeds = model.get_video_features(**inputs).cpu().numpy()\n",
    "\n",
    "        embedding_batches.append(embeds)\n",
    "        for m in current_meta:\n",
    "            lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "    # Concatenate & save\n",
    "    all_embeddings = np.vstack(embedding_batches)\n",
    "    print(f\"Saving {all_embeddings.shape[0]} embeddings to disk...\")\n",
    "    np.save(os.path.join(output_dir, \"video_embeddings.npy\"), all_embeddings)\n",
    "    with open(os.path.join(output_dir, \"embedding_lookup.json\"), \"w\") as f:\n",
    "        json.dump(lookup_dict, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved embeddings to {output_dir}/video_embeddings.npy\")\n",
    "    print(f\"✅ Saved lookup to  {output_dir}/embedding_lookup.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_directory_to_embeddings(VIDEO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776706a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dean/code/miniconda3/envs/dedup/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dean/code/miniconda3/envs/dedup/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/Users/dean/code/miniconda3/envs/dedup/lib/python3.9/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import av\n",
    "import cv2\n",
    "from transformers import AutoModel\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "VIDEO_DIR       = \"input_videos\"\n",
    "OUTPUT_DIR      = \"output_embeddings\"\n",
    "FRAMES_PER_CLIP = 12\n",
    "FRAME_SAMPLE    = 8   # Unused here, but can be used to subsample frames\n",
    "BATCH_SIZE      = 8\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    device = \"cuda:1\"\n",
    "    print(f\"device:{DEVICE}\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"device:{device}\")\n",
    "else:\n",
    "    print(f\"Plain ol' CPU\")\n",
    "\n",
    "# ==== LOAD ViCLIP ====\n",
    "model = AutoModel.from_pretrained(\"OpenGVLab/ViCLIP-L-14-hf\", trust_remote_code=True).to(device).eval()\n",
    "\n",
    "\n",
    "# ==== FRAME EXTRACTOR ====\n",
    "def read_video_clip(container, indices, fps):\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        ts = int((idx / fps) * 1e6)\n",
    "        container.seek(ts, any_frame=False, backward=True)\n",
    "        for frame in container.decode(video=0):\n",
    "            frames.append(frame.to_ndarray(format=\"rgb24\"))\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "\n",
    "# ==== FRAME NORMALIZATION + TENSOR CONVERSION ====\n",
    "v_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
    "v_std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n",
    "\n",
    "def normalize(data):\n",
    "    return (data / 255.0 - v_mean) / v_std\n",
    "\n",
    "def frames2tensor(vid_list, fnum=8, target_size=(224, 224), device=torch.device('cuda')):\n",
    "    assert len(vid_list) >= fnum\n",
    "    step = len(vid_list) // fnum\n",
    "    vid_list = vid_list[::step][:fnum]\n",
    "    vid_list = [cv2.resize(x[:, :, ::-1], target_size) for x in vid_list]\n",
    "    vid_tube = [np.expand_dims(normalize(x), axis=(0, 1)) for x in vid_list]\n",
    "    vid_tube = np.concatenate(vid_tube, axis=1)  # (1, T, H, W, C)\n",
    "    vid_tube = np.transpose(vid_tube, (0, 1, 4, 2, 3))  # (1, T, C, H, W)\n",
    "    vid_tube = torch.from_numpy(vid_tube).to(device, non_blocking=True).float()\n",
    "    return vid_tube\n",
    "\n",
    "\n",
    "# ==== BATCH PROCESSING FUNCTION ====\n",
    "def process_directory_to_embeddings(\n",
    "    video_dir: str,\n",
    "    clip_len: int = FRAMES_PER_CLIP,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    output_dir: str = OUTPUT_DIR,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    embedding_batches = []\n",
    "    lookup_dict = {}\n",
    "    global_clip_id = 0\n",
    "    current_batch = []\n",
    "    current_meta = []\n",
    "\n",
    "    exts = ('.mp4', '.mov', '.avi')\n",
    "\n",
    "    for filename in sorted(os.listdir(video_dir)):\n",
    "        if not filename.lower().endswith(exts):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_dir, filename)\n",
    "        container = av.open(video_path)\n",
    "        stream = container.streams.video[0]\n",
    "        total_frames = stream.frames\n",
    "        fps = float(stream.average_rate) if stream.average_rate else 1.0\n",
    "\n",
    "        clip_index = 0\n",
    "        window_size = int(10 * fps)\n",
    "\n",
    "        for start in range(0, total_frames - window_size + 1, window_size):\n",
    "            indices = np.linspace(start, start + window_size, num=clip_len, endpoint=False, dtype=np.int64)\n",
    "            frames = read_video_clip(container, indices, fps)\n",
    "            if len(frames) < clip_len:\n",
    "                continue\n",
    "\n",
    "            current_batch.append(frames)\n",
    "            current_meta.append({\n",
    "                \"clip_id\":        global_clip_id,\n",
    "                \"clip_index\":     clip_index,\n",
    "                \"video_file\":     filename,\n",
    "                \"start_time_sec\": int(start / fps),\n",
    "            })\n",
    "\n",
    "            clip_index += 1\n",
    "            global_clip_id += 1\n",
    "\n",
    "            if len(current_batch) == batch_size:\n",
    "                print(f\"Running inference on batch of {batch_size} clips: IDs {current_meta[0]['clip_id']}-{current_meta[-1]['clip_id']}\")\n",
    "                batch_tensor = torch.cat([frames2tensor(frames, fnum=clip_len, device=device) for frames in current_batch], dim=0)\n",
    "                with torch.no_grad():\n",
    "                    embeds = model.get_vid_features(batch_tensor).cpu().numpy()\n",
    "\n",
    "                embedding_batches.append(embeds)\n",
    "                for m in current_meta:\n",
    "                    lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "                current_batch = []\n",
    "                current_meta = []\n",
    "\n",
    "        container.close()\n",
    "\n",
    "    # Final batch\n",
    "    if current_batch:\n",
    "        print(f\"Running inference on final batch of {len(current_batch)} clips: IDs {current_meta[0]['clip_id']}-{current_meta[-1]['clip_id']}\")\n",
    "        batch_tensor = torch.cat([frames2tensor(frames, fnum=clip_len, device=device) for frames in current_batch], dim=0)\n",
    "        with torch.no_grad():\n",
    "            embeds = model.get_vid_features(batch_tensor).cpu().numpy()\n",
    "        embedding_batches.append(embeds)\n",
    "        for m in current_meta:\n",
    "            lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "    all_embeddings = np.vstack(embedding_batches)\n",
    "    np.save(os.path.join(output_dir, \"video_embeddings.npy\"), all_embeddings)\n",
    "    with open(os.path.join(output_dir, \"embedding_lookup.json\"), \"w\") as f:\n",
    "        json.dump(lookup_dict, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved embeddings to {output_dir}/video_embeddings.npy\")\n",
    "    print(f\"✅ Saved lookup to  {output_dir}/embedding_lookup.json\")\n",
    "\n",
    "\n",
    "# ==== MAIN ENTRY ====\n",
    "if __name__ == \"__main__\":\n",
    "    process_directory_to_embeddings(VIDEO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c43c8",
   "metadata": {},
   "source": [
    "## Index in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28beaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────────────────\n",
    "EMB_PATH    = \"output/video_embeddings.npy\"\n",
    "LOOKUP_PATH = \"output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"output/video_embeddings.index\"\n",
    "# ────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# 1️⃣ Load embeddings and lookup dict\n",
    "embeddings = np.load(EMB_PATH)  \n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# 2️⃣ Build FAISS index (inner-product) and add IDs\n",
    "faiss.normalize_L2(embeddings)\n",
    "dim   = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "ids = np.arange(embeddings.shape[0], dtype=\"int64\")\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "# (Optional) save the index for later\n",
    "faiss.write_index(index, INDEX_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4f644",
   "metadata": {},
   "source": [
    "## Find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e27c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "\n",
    "EMB_PATH    = \"output/video_embeddings.npy\"\n",
    "LOOKUP_PATH = \"output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"output/video_embeddings.index\"\n",
    "\n",
    "\n",
    "def find_similar_filenames(radius: float):\n",
    "    \"\"\"\n",
    "    Load embeddings, lookup dict, and FAISS index, then perform a range search\n",
    "    to find all unique pairs of different video filenames whose clip embeddings\n",
    "    lie within the given radius (cosine similarity threshold).\n",
    "    \"\"\"\n",
    "    # Load embeddings and metadata\n",
    "    embeddings = np.load(EMB_PATH)\n",
    "    with open(LOOKUP_PATH, \"r\") as f:\n",
    "        lookup = json.load(f)\n",
    "\n",
    "    # Read the FAISS index\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "    # Ensure embeddings are normalized (for cosine similarity via inner product)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    # Perform range search\n",
    "    lims, distances, labels = index.range_search(embeddings, radius)\n",
    "\n",
    "    pairs = set()\n",
    "    # Iterate over each query embedding\n",
    "    for q in range(len(embeddings)):\n",
    "        start, end = lims[q], lims[q+1]\n",
    "        for idx in labels[start:end]:\n",
    "            if idx <= q:\n",
    "                # skip self and symmetric duplicates\n",
    "                continue\n",
    "            f1 = lookup[str(q)][\"video_file\"]\n",
    "            f2 = lookup[str(idx)][\"video_file\"]\n",
    "            if f1 != f2:\n",
    "                # add sorted tuple to avoid duplicate orderings\n",
    "                pairs.add(tuple(sorted((f1, f2))))\n",
    "    return pairs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose your similarity radius (e.g., 0.8 for cosine similarity >= 0.8)\n",
    "    radius = 0.999\n",
    "\n",
    "    similar_pairs = find_similar_filenames(radius)\n",
    "    print(\"Similar file pairs within radius\", radius, \":\")\n",
    "    for a, b in sorted(similar_pairs):\n",
    "        print(f\"{a} <--> {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5094e",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload your positional name_list\n",
    "with open(\"clip_name_list.pkl\", \"rb\") as f:\n",
    "    name_dict = pickle.load(f)\n",
    "\n",
    "emb_matrix = np.load(EMB_FILE)[:len(name_dict)]\n",
    "\n",
    "# Build metadata from name_list\n",
    "metadata = [ f\"{fn}[{idx}]\" for fn, idx in name_dict ]\n",
    "\n",
    "# Sanity check lengths match\n",
    "assert emb_matrix.shape[0] == len(metadata), (\n",
    "    f\"❌ {emb_matrix.shape[0]} embeddings vs {len(metadata)} metadata entries\"\n",
    ")\n",
    "\n",
    "# 5) Write to TensorBoard\n",
    "writer = SummaryWriter(log_dir=\"runs/embeds\")\n",
    "writer.add_embedding(\n",
    "    emb_matrix,\n",
    "    metadata=metadata,\n",
    "    tag=\"my_embeddings\"\n",
    ")\n",
    "writer.close()\n",
    "\n",
    "print(\"Done. Run:\\n  tensorboard --logdir=runs/embeds\\nThen open http://localhost:6006/#projector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09882c39",
   "metadata": {},
   "source": [
    "## Search prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "LOOKUP_PATH = \"output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"output/video_embeddings.index\"\n",
    "MODEL_CHECKPOINT = \"microsoft/xclip-base-patch32\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Load lookup table and FAISS index\n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "# Load tokenizer & text model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModel.from_pretrained(MODEL_CHECKPOINT).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def search_prompts(prompts, top_k=1):\n",
    "    \"\"\"\n",
    "    Encode text prompts, search the FAISS index, and return\n",
    "    filename + timestamp for each top-k match.\n",
    "    \"\"\"\n",
    "    # Tokenize and encode\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        text_feats = model.get_text_features(**inputs)\n",
    "    text_feats = text_feats.cpu().numpy()\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(text_feats)\n",
    "\n",
    "    # Search\n",
    "    D, I = index.search(text_feats, top_k)\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        matches = []\n",
    "        for score, clip_id in zip(D[i], I[i]):\n",
    "            info = lookup[str(int(clip_id))]\n",
    "            matches.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"file\": info[\"video_file\"],\n",
    "                \"start_time_sec\": info[\"start_time_sec\"],\n",
    "                \"clip_index\": info[\"clip_index\"],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        results.append(matches)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompts = [\n",
    "        \"Videos of a man injured in the daytime. Smoke is rising in the background\",\n",
    "        \"A clown eating a huge bowl of spagetti while riding a bicycle\"\n",
    "    ]\n",
    "    results = search_prompts(prompts, top_k=3)\n",
    "    for match_list in results:\n",
    "        for match in match_list:\n",
    "            print(f\"Prompt: {match['prompt']}\")\n",
    "            print(f\"  File: {match['file']}\")\n",
    "            print(f\"  Start time: {match['start_time_sec']}s (clip index {match['clip_index']})\")\n",
    "            print(f\"  Similarity: {match['similarity']:.4f}\")\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dedup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
