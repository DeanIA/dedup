{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fd6c74",
   "metadata": {},
   "source": [
    "## Import & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55fa6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import av\n",
    "import cv2\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "import re\n",
    "import faiss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "VIDEO_DIR          = \"input_videos\"\n",
    "OUTPUT_DIR         = \"xclip_output\"\n",
    "SECONDS_PER_CLIP   = 30        # ▶ clip length in seconds (change this variable)\n",
    "CLIP_INTERVAL_SEC  = 10       # ▶ time between clip starts, in seconds\n",
    "CLIP_LEN           = 8       # ▶ how many frames to sample per clip\n",
    "BATCH_SIZE         = 8\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    device = \"cuda:0\"\n",
    "    print(f\"device:{device}\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"device:{device}\")\n",
    "else:\n",
    "    print(f\"Plain ol' CPU\")\n",
    "\n",
    "# ==== XCLIP SETUP ====\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch16\")  #  [oai_citation:0‡huggingface.co](https://huggingface.co/microsoft/xclip-base-patch16?utm_source=chatgpt.com)\n",
    "model     = AutoModel.from_pretrained(\"microsoft/xclip-base-patch16\").to(device).eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccda9d",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776706a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing clips 0–7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.local/lib/python3.10/site-packages/transformers/image_processing_utils.py:42: UserWarning: The following named arguments are not valid for `VideoMAEImageProcessor.preprocess` and were ignored: 'padding'\n",
      "  return self.preprocess(images, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing clips 8–15\n",
      "Inferencing clips 16–23\n",
      "Inferencing clips 24–31\n",
      "Inferencing clips 32–39\n",
      "Inferencing clips 40–47\n",
      "Inferencing clips 48–55\n",
      "Inferencing clips 56–63\n",
      "Inferencing clips 64–71\n",
      "Inferencing clips 72–79\n",
      "Inferencing clips 80–87\n",
      "Inferencing clips 88–95\n",
      "Inferencing clips 96–103\n",
      "Inferencing clips 104–111\n",
      "Inferencing clips 112–119\n",
      "Inferencing clips 120–127\n",
      "Inferencing clips 128–135\n",
      "Inferencing clips 136–143\n",
      "Inferencing clips 144–151\n",
      "Inferencing clips 152–159\n",
      "Inferencing clips 160–167\n",
      "Inferencing clips 168–175\n",
      "Inferencing clips 176–183\n",
      "Inferencing clips 184–191\n",
      "Inferencing clips 192–199\n",
      "Inferencing clips 200–207\n",
      "Inferencing clips 208–215\n",
      "Inferencing clips 216–223\n",
      "Inferencing clips 224–231\n",
      "Inferencing clips 232–239\n",
      "Inferencing clips 240–247\n",
      "Inferencing clips 248–255\n",
      "Inferencing clips 256–263\n",
      "Inferencing clips 264–271\n",
      "Inferencing clips 272–279\n",
      "Inferencing clips 280–287\n",
      "Inferencing clips 288–295\n",
      "Inferencing clips 296–303\n",
      "Inferencing clips 304–311\n",
      "Inferencing clips 312–319\n",
      "Inferencing clips 320–327\n",
      "Inferencing clips 328–335\n",
      "Inferencing clips 336–343\n",
      "Inferencing clips 344–351\n",
      "Inferencing clips 352–359\n",
      "Inferencing clips 360–367\n",
      "Inferencing clips 368–375\n",
      "Inferencing clips 376–383\n",
      "Inferencing clips 384–391\n",
      "Inferencing clips 392–399\n",
      "Inferencing clips 400–407\n",
      "Inferencing clips 408–415\n",
      "Inferencing clips 416–423\n",
      "Inferencing clips 424–431\n",
      "Inferencing clips 432–439\n",
      "Inferencing clips 440–447\n",
      "Inferencing clips 448–455\n",
      "Inferencing clips 456–463\n",
      "Inferencing clips 464–471\n",
      "Inferencing clips 472–479\n",
      "Inferencing clips 480–487\n",
      "Inferencing clips 488–495\n",
      "Inferencing clips 496–503\n",
      "Inferencing clips 504–511\n",
      "Inferencing final 3 clips\n",
      "✅ Saved 515 embeddings to xclip_output/video_embeddings.npy\n",
      "✅ Saved lookup   to xclip_output/embedding_lookup.json\n"
     ]
    }
   ],
   "source": [
    "# ==== FRAME EXTRACTOR ====\n",
    "def read_video_clip(container, indices, fps):\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        ts = int((idx / fps) * 1e6)\n",
    "        container.seek(ts, any_frame=False, backward=True)\n",
    "        for frame in container.decode(video=0):\n",
    "            frames.append(frame.to_ndarray(format=\"rgb24\"))\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "# ==== BATCH PROCESSING ====\n",
    "def process_directory_to_embeddings(\n",
    "    video_dir: str,\n",
    "    clip_len: int = CLIP_LEN,\n",
    "    seconds_per_clip: int = SECONDS_PER_CLIP,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    output_dir: str = OUTPUT_DIR,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    embedding_batches = []\n",
    "    lookup_dict = {}\n",
    "    global_clip_id = 0\n",
    "    current_batch = []\n",
    "    current_meta = []\n",
    "\n",
    "    for filename in sorted(os.listdir(video_dir)):\n",
    "        if not filename.lower().endswith(('.mp4', '.mov', '.avi', 'wav', 'webm')):\n",
    "            continue\n",
    "\n",
    "        path      = os.path.join(video_dir, filename)\n",
    "        container = av.open(path)\n",
    "        stream    = container.streams.video[0]\n",
    "        fps       = float(stream.average_rate) if stream.average_rate else 30.0\n",
    "        total_f   = stream.frames\n",
    "        window    = int(fps * seconds_per_clip)\n",
    "\n",
    "        clip_index = 0\n",
    "        for start in range(0, total_f - window + 1, window):\n",
    "            indices = np.linspace(\n",
    "                start, start + window, num=clip_len, endpoint=False, dtype=np.int64\n",
    "            )\n",
    "            frames = read_video_clip(container, indices, fps)\n",
    "            if len(frames) < clip_len:\n",
    "                continue\n",
    "\n",
    "            current_batch.append(frames)\n",
    "            current_meta.append({\n",
    "                \"clip_id\":        global_clip_id,\n",
    "                \"clip_index\":     clip_index,\n",
    "                \"video_file\":     filename,\n",
    "                \"start_time_sec\": int(start / fps),\n",
    "            })\n",
    "            clip_index += 1\n",
    "            global_clip_id += 1\n",
    "\n",
    "            if len(current_batch) == batch_size:\n",
    "                print(f\"Inferencing clips {current_meta[0]['clip_id']}–{current_meta[-1]['clip_id']}\")\n",
    "                inputs = processor(videos=current_batch, return_tensors=\"pt\", padding=True)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    embeds = model.get_video_features(**inputs).cpu().numpy()\n",
    "\n",
    "                embedding_batches.append(embeds)\n",
    "                for m in current_meta:\n",
    "                    lookup_dict[str(m[\"clip_id\"])] = m\n",
    "                current_batch = []\n",
    "                current_meta = []\n",
    "\n",
    "        container.close()\n",
    "\n",
    "    # Final batch\n",
    "    if current_batch:\n",
    "        print(f\"Inferencing final {len(current_batch)} clips\")\n",
    "        inputs = processor(videos=current_batch, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            embeds = model.get_video_features(**inputs).cpu().numpy()\n",
    "        embedding_batches.append(embeds)\n",
    "        for m in current_meta:\n",
    "            lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "    # Save embeddings + lookup\n",
    "    all_embeddings = np.vstack(embedding_batches)\n",
    "    np.save(os.path.join(output_dir, \"video_embeddings.npy\"), all_embeddings)\n",
    "    with open(os.path.join(output_dir, \"embedding_lookup.json\"), \"w\") as f:\n",
    "        json.dump(lookup_dict, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved {all_embeddings.shape[0]} embeddings to {output_dir}/video_embeddings.npy\")\n",
    "    print(f\"✅ Saved lookup   to {output_dir}/embedding_lookup.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_directory_to_embeddings(VIDEO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5094e",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab54de12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
      "✅ Done. Run:\n",
      "  tensorboard --logdir=runs/embeds\n",
      "Then open http://localhost:6006/#projector\n"
     ]
    }
   ],
   "source": [
    "# Load lookup metadata\n",
    "with open(\"xclip_output/embedding_lookup.json\", \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Ensure entries are sorted by clip_id (assumed to match row index)\n",
    "sorted_lookup = sorted(lookup.values(), key=lambda x: int(x[\"clip_id\"]))\n",
    "\n",
    "# Create metadata strings like: \"video.mp4[3] @ 90s\"\n",
    "metadata = [\n",
    "    f'{entry[\"video_file\"]}[{entry[\"clip_index\"]}] @ {entry[\"start_time_sec\"]}s'\n",
    "    for entry in sorted_lookup\n",
    "]\n",
    "\n",
    "# Load embedding matrix\n",
    "emb_matrix = np.load(\"xclip_output/video_embeddings.npy\")\n",
    "\n",
    "# Ensure alignment\n",
    "assert emb_matrix.shape[0] == len(metadata), (\n",
    "    f\"❌ {emb_matrix.shape[0]} embeddings vs {len(metadata)} metadata entries\"\n",
    ")\n",
    "\n",
    "# Write to TensorBoard\n",
    "writer = SummaryWriter(log_dir=\"runs/embeds\")\n",
    "writer.add_embedding(\n",
    "    emb_matrix,\n",
    "    metadata=metadata,\n",
    "    tag=\"my_embeddings\"\n",
    ")\n",
    "writer.close()\n",
    "\n",
    "print(\"✅ Done. Run:\\n  tensorboard --logdir=runs/embeds\\nThen open http://localhost:6006/#projector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c43c8",
   "metadata": {},
   "source": [
    "## Index in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28beaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH    = \"xclip_output/video_embeddings.npy\"\n",
    "LOOKUP_PATH = \"xclip_output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"xclip_output/video_embeddings.index\"\n",
    "\n",
    "# Load embeddings and lookup dict\n",
    "embeddings = np.load(EMB_PATH)  \n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Build FAISS index (inner-product) and add IDs\n",
    "faiss.normalize_L2(embeddings)\n",
    "dim   = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "ids = np.arange(embeddings.shape[0], dtype=\"int64\")\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "# Save the index for later\n",
    "faiss.write_index(index, INDEX_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14c7f8",
   "metadata": {},
   "source": [
    "## Find duplicates and optimal radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b630e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   radius  true_positives  false_positives  false_negatives        recall\n",
      "0   0.900               3               28          4685307  6.402991e-07\n",
      "1   0.950               2               10          4685308  4.268661e-07\n",
      "2   0.990               2                0          4685308  4.268661e-07\n",
      "3   0.999               2                0          4685308  4.268661e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to embeddings, lookup, and index\n",
    "EMB_PATH    = 'videoclip_output/video_embeddings.npy'\n",
    "LOOKUP_PATH = 'videoclip_output/embedding_lookup.json'\n",
    "INDEX_PATH  = 'videoclip_output/video_embeddings.index'\n",
    "\n",
    "def count_similar_clip_pairs(radius):\n",
    "    embeddings = np.load(EMB_PATH)\n",
    "    with open(LOOKUP_PATH) as f:\n",
    "        lookup = json.load(f)\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    lims, distances, labels = index.range_search(embeddings, radius)\n",
    "\n",
    "    pair_stats = defaultdict(list)\n",
    "    for q in range(len(embeddings)):\n",
    "        for i in range(lims[q], lims[q+1]):\n",
    "            idx = labels[i]\n",
    "            if idx <= q:\n",
    "                continue\n",
    "            f1 = lookup[str(q)]['video_file']\n",
    "            f2 = lookup[str(idx)]['video_file']\n",
    "            if f1 != f2:\n",
    "                pair = tuple(sorted((f1, f2)))\n",
    "                pair_stats[pair].append(distances[i])\n",
    "\n",
    "    return [\n",
    "        (f1, f2, len(dists), sum(dists)/len(dists))\n",
    "        for (f1, f2), dists in pair_stats.items()\n",
    "    ]\n",
    "\n",
    "def evaluate_duplicates(pairs, ground_truth):\n",
    "    def extract_id(name):\n",
    "        m = re.search(r'TNS_(\\d+)', name)\n",
    "        return m.group(1) if m else None\n",
    "\n",
    "    detected = {\n",
    "        tuple(sorted((extract_id(f1), extract_id(f2))))\n",
    "        for f1, f2, _, _ in pairs\n",
    "    }\n",
    "    truth_dups = {\n",
    "        extract_id(k)\n",
    "        for k, v in ground_truth.items()\n",
    "        if v.upper() == 'DUPLICATE'\n",
    "    }\n",
    "    truth_pairs = {\n",
    "        tuple(sorted((dup, extract_id(pr))))\n",
    "        for dup in truth_dups\n",
    "        for pr, st in ground_truth.items()\n",
    "        if st.upper() == 'PRINCIPAL'\n",
    "    }\n",
    "\n",
    "    tp = len(detected & truth_pairs)\n",
    "    fp = len(detected - truth_pairs)\n",
    "    fn = len(truth_pairs - detected)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# --- Load ground truth ---\n",
    "gt_df = pd.read_csv('dup_groundtruth.csv')  # comma-separated file\n",
    "ground_truth = dict(zip(gt_df['UAR Code'], gt_df['Principal / Duplicate']))\n",
    "\n",
    "# --- Loop over different radii ---\n",
    "radii = [0.9, 0.95, 0.99, 0.999]\n",
    "results = []\n",
    "\n",
    "for r in radii:\n",
    "    pairs   = count_similar_clip_pairs(r)\n",
    "    metrics = evaluate_duplicates(pairs, ground_truth)\n",
    "    results.append({'radius': r, **metrics})\n",
    "\n",
    "# --- Export & display ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('evaluation_results.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09882c39",
   "metadata": {},
   "source": [
    "## Search prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "LOOKUP_PATH = \"output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"output/video_embeddings.index\"\n",
    "MODEL_CHECKPOINT = \"microsoft/xclip-base-patch32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Load lookup table and FAISS index\n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "# Load tokenizer & text model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModel.from_pretrained(MODEL_CHECKPOINT).to(device)\n",
    "model.eval()\n",
    "\n",
    "def search_prompts(prompts, top_k=1):\n",
    "    \"\"\"\n",
    "    Encode text prompts, search the FAISS index, and return\n",
    "    filename + timestamp for each top-k match.\n",
    "    \"\"\"\n",
    "    # Tokenize and encode\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        text_feats = model.get_text_features(**inputs)\n",
    "    text_feats = text_feats.cpu().numpy()\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(text_feats)\n",
    "\n",
    "    # Search\n",
    "    D, I = index.search(text_feats, top_k)\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        matches = []\n",
    "        for score, clip_id in zip(D[i], I[i]):\n",
    "            info = lookup[str(int(clip_id))]\n",
    "            matches.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"file\": info[\"video_file\"],\n",
    "                \"start_time_sec\": info[\"start_time_sec\"],\n",
    "                \"clip_index\": info[\"clip_index\"],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        results.append(matches)\n",
    "    return results\n",
    "\n",
    "prompts = [\n",
    "    \"Videos of a man injured in the daytime. Smoke is rising in the background\",\n",
    "    \"A clown eating a huge bowl of spagetti while riding a bicycle\"\n",
    "]\n",
    "results = search_prompts(prompts, top_k=3)\n",
    "for match_list in results:\n",
    "    for match in match_list:\n",
    "        print(f\"Prompt: {match['prompt']}\")\n",
    "        print(f\"  File: {match['file']}\")\n",
    "        print(f\"  Start time: {match['start_time_sec']}s (clip index {match['clip_index']})\")\n",
    "        print(f\"  Similarity: {match['similarity']:.4f}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dedup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
