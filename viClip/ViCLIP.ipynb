{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fd6c74",
   "metadata": {},
   "source": [
    "## Import & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55fa6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/jupyter-dai7591/.cache/huggingface/modules/transformers_modules/OpenGVLab/ViCLIP-L-14-hf/1652361522e1cb41c28cdfae870f690d00e7456b/viclip_text.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import av\n",
    "import cv2\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import faiss\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "VIDEO_DIR       = \"input_videos\"\n",
    "OUTPUT_DIR      = \"videoclip_output\"\n",
    "FRAMES_PER_CLIP = 8\n",
    "CLIP_DURATION   = 30\n",
    "BATCH_SIZE      = 128\n",
    "\n",
    "if torch.cuda.is_available() : \n",
    "    device = \"cuda:0\"\n",
    "    print(f\"device:{device}\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"device:{device}\")\n",
    "else:\n",
    "    print(f\"Plain ol' CPU\")\n",
    "\n",
    "# ==== LOAD ViCLIP ====\n",
    "model = AutoModel.from_pretrained(\"OpenGVLab/ViCLIP-L-14-hf\", trust_remote_code=True).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccda9d",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776706a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on batch of 128 clips: IDs 0-127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on batch of 128 clips: IDs 128-255\n",
      "Running inference on batch of 128 clips: IDs 256-383\n",
      "Running inference on batch of 128 clips: IDs 384-511\n",
      "Running inference on batch of 128 clips: IDs 512-639\n",
      "Running inference on batch of 128 clips: IDs 640-767\n",
      "Running inference on batch of 128 clips: IDs 768-895\n",
      "Running inference on batch of 128 clips: IDs 896-1023\n",
      "Running inference on batch of 128 clips: IDs 1024-1151\n",
      "Running inference on batch of 128 clips: IDs 1152-1279\n",
      "Running inference on batch of 128 clips: IDs 1280-1407\n",
      "Running inference on batch of 128 clips: IDs 1408-1535\n",
      "Running inference on final batch of 113 clips: IDs 1536-1648\n",
      "âœ… Saved embeddings to videoclip_output/video_embeddings.npy\n",
      "âœ… Saved lookup to  videoclip_output/embedding_lookup.json\n"
     ]
    }
   ],
   "source": [
    "# ==== FRAME EXTRACTOR ====\n",
    "def read_video_clip(container, indices, fps):\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        ts = int((idx / fps) * 1e6)\n",
    "        container.seek(ts, any_frame=False, backward=True)\n",
    "        for frame in container.decode(video=0):\n",
    "            frames.append(frame.to_ndarray(format=\"rgb24\"))\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "\n",
    "# ==== FRAME NORMALIZATION + TENSOR CONVERSION ====\n",
    "v_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
    "v_std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n",
    "\n",
    "def normalize(data):\n",
    "    return (data / 255.0 - v_mean) / v_std\n",
    "\n",
    "def frames2tensor(vid_list, fnum=8, target_size=(224, 224), device=torch.device('cuda')):\n",
    "    assert len(vid_list) >= fnum\n",
    "    step = len(vid_list) // fnum\n",
    "    vid_list = vid_list[::step][:fnum]\n",
    "    vid_list = [cv2.resize(x[:, :, ::-1], target_size) for x in vid_list]\n",
    "    vid_tube = [np.expand_dims(normalize(x), axis=0) for x in vid_list]  # (1, H, W, C) for each frame\n",
    "    vid_tube = np.stack(vid_tube, axis=1)  # (1, T, H, W, C)\n",
    "    vid_tube = np.transpose(vid_tube, (0, 1, 4, 2, 3))  # (1, T, C, H, W)\n",
    "    vid_tube = torch.from_numpy(vid_tube).to(device, non_blocking=True).float()\n",
    "    return vid_tube\n",
    "\n",
    "\n",
    "# ==== BATCH PROCESSING FUNCTION ====\n",
    "def process_directory_to_embeddings(\n",
    "    video_dir: str,\n",
    "    clip_len: int = 8,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    output_dir: str = OUTPUT_DIR,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    embedding_batches = []\n",
    "    lookup_dict = {}\n",
    "    global_clip_id = 0\n",
    "    current_batch = []\n",
    "    current_meta = []\n",
    "\n",
    "    exts = ('.mp4', '.mov', '.avi', 'wav', 'webm')\n",
    "\n",
    "    for filename in sorted(os.listdir(video_dir)):\n",
    "        if not filename.lower().endswith(exts):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_dir, filename)\n",
    "        container = av.open(video_path)\n",
    "        stream = container.streams.video[0]\n",
    "        total_frames = stream.frames\n",
    "        fps = float(stream.average_rate) if stream.average_rate else 1.0\n",
    "\n",
    "        clip_index = 0\n",
    "        window_size = int(10 * fps)\n",
    "\n",
    "        for start in range(0, total_frames - window_size + 1, window_size):\n",
    "            indices = np.linspace(start, start + window_size, num=clip_len, endpoint=False, dtype=np.int64)\n",
    "            frames = read_video_clip(container, indices, fps)\n",
    "            if len(frames) < clip_len:\n",
    "                continue\n",
    "\n",
    "            current_batch.append(frames)\n",
    "            current_meta.append({\n",
    "                \"clip_id\":        global_clip_id,\n",
    "                \"clip_index\":     clip_index,\n",
    "                \"video_file\":     filename,\n",
    "                \"start_time_sec\": int(start / fps),\n",
    "            })\n",
    "\n",
    "            clip_index += 1\n",
    "            global_clip_id += 1\n",
    "\n",
    "            if len(current_batch) == batch_size:\n",
    "                print(f\"Running inference on batch of {batch_size} clips: IDs {current_meta[0]['clip_id']}-{current_meta[-1]['clip_id']}\")\n",
    "                batch_tensor = torch.cat([frames2tensor(frames, fnum=clip_len, device=device) for frames in current_batch], dim=0)\n",
    "                with torch.no_grad():\n",
    "                    embeds = model.get_vid_features(batch_tensor).cpu().numpy()\n",
    "\n",
    "                embedding_batches.append(embeds)\n",
    "                for m in current_meta:\n",
    "                    lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "                current_batch = []\n",
    "                current_meta = []\n",
    "\n",
    "        container.close()\n",
    "\n",
    "    # Final batch\n",
    "    if current_batch:\n",
    "        print(f\"Running inference on final batch of {len(current_batch)} clips: IDs {current_meta[0]['clip_id']}-{current_meta[-1]['clip_id']}\")\n",
    "        batch_tensor = torch.cat([frames2tensor(frames, fnum=clip_len, device=device) for frames in current_batch], dim=0)\n",
    "        with torch.no_grad():\n",
    "            embeds = model.get_vid_features(batch_tensor).cpu().numpy()\n",
    "        embedding_batches.append(embeds)\n",
    "        for m in current_meta:\n",
    "            lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "    all_embeddings = np.vstack(embedding_batches)\n",
    "    np.save(os.path.join(output_dir, \"video_embeddings.npy\"), all_embeddings)\n",
    "    with open(os.path.join(output_dir, \"embedding_lookup.json\"), \"w\") as f:\n",
    "        json.dump(lookup_dict, f, indent=2)\n",
    "\n",
    "    print(f\"âœ… Saved embeddings to {output_dir}/video_embeddings.npy\")\n",
    "    print(f\"âœ… Saved lookup to  {output_dir}/embedding_lookup.json\")\n",
    "\n",
    "\n",
    "# ==== MAIN ENTRY ====\n",
    "if __name__ == \"__main__\":\n",
    "    process_directory_to_embeddings(VIDEO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5094e",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab54de12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done. Run:\n",
      "  tensorboard --logdir=runs/embeds\n",
      "Then open http://localhost:6006/#projector\n"
     ]
    }
   ],
   "source": [
    "# Load lookup metadata\n",
    "with open(\"videoclip_output/embedding_lookup.json\", \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Ensure entries are sorted by clip_id (assumed to match row index)\n",
    "sorted_lookup = sorted(lookup.values(), key=lambda x: int(x[\"clip_id\"]))\n",
    "\n",
    "# Create metadata strings like: \"video.mp4[3] @ 90s\"\n",
    "metadata = [\n",
    "    f'{entry[\"video_file\"]}[{entry[\"clip_index\"]}] @ {entry[\"start_time_sec\"]}s'\n",
    "    for entry in sorted_lookup\n",
    "]\n",
    "\n",
    "# Load embedding matrix\n",
    "emb_matrix = np.load(\"videoclip_output/video_embeddings.npy\")\n",
    "\n",
    "# Ensure alignment\n",
    "assert emb_matrix.shape[0] == len(metadata), (\n",
    "    f\"âŒ {emb_matrix.shape[0]} embeddings vs {len(metadata)} metadata entries\"\n",
    ")\n",
    "\n",
    "# Write to TensorBoard\n",
    "writer = SummaryWriter(log_dir=\"runs/embeds\")\n",
    "writer.add_embedding(\n",
    "    emb_matrix,\n",
    "    metadata=metadata,\n",
    "    tag=\"my_embeddings\"\n",
    ")\n",
    "writer.close()\n",
    "\n",
    "print(\"âœ… Done. Run:\\n  tensorboard --logdir=runs/embeds\\nThen open http://localhost:6006/#projector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c43c8",
   "metadata": {},
   "source": [
    "## Index in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28beaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH    = \"videoclip_output/video_embeddings.npy\"\n",
    "LOOKUP_PATH = \"videoclip_output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"videoclip_output/video_embeddings.index\"\n",
    "\n",
    "# Load embeddings and lookup dict\n",
    "embeddings = np.load(EMB_PATH)  \n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Build FAISS index (inner-product) and add IDs\n",
    "faiss.normalize_L2(embeddings)\n",
    "dim   = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "ids = np.arange(embeddings.shape[0], dtype=\"int64\")\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "# Save the index for later\n",
    "faiss.write_index(index, INDEX_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4f644",
   "metadata": {},
   "source": [
    "## Find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e27c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNS_0030_V.mp4 <--> TNS_0031_V.mp4 | similar clips: 205 | avg distance: 1.0000\n",
      "TNS_0024_V.mp4 <--> TNS_0025_V.mp4 | similar clips: 59 | avg distance: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "def count_similar_clip_pairs(radius: float):\n",
    "    \"\"\"\n",
    "    For each unique pair of video files, count how many clip pairs are similar\n",
    "    (i.e., within the given radius), and record the average distance.\n",
    "    Returns: list of (file1, file2, count, avg_distance)\n",
    "    \"\"\"\n",
    "    embeddings = np.load(EMB_PATH)\n",
    "    with open(LOOKUP_PATH, \"r\") as f:\n",
    "        lookup = json.load(f)\n",
    "\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    lims, distances, labels = index.range_search(embeddings, radius)\n",
    "\n",
    "    # Map (file1, file2) to list of distances\n",
    "    pair_stats = defaultdict(list)\n",
    "\n",
    "    for q in range(len(embeddings)):\n",
    "        start, end = lims[q], lims[q+1]\n",
    "        for i in range(start, end):\n",
    "            idx = labels[i]\n",
    "            if idx <= q:\n",
    "                continue  # skip self or repeated\n",
    "            f1 = lookup[str(q)][\"video_file\"]\n",
    "            f2 = lookup[str(idx)][\"video_file\"]\n",
    "            if f1 != f2:\n",
    "                key = tuple(sorted((f1, f2)))\n",
    "                pair_stats[key].append(distances[i])\n",
    "\n",
    "    results = []\n",
    "    for (f1, f2), dists in pair_stats.items():\n",
    "        avg_dist = sum(dists) / len(dists)\n",
    "        results.append((f1, f2, len(dists), avg_dist))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Choose your similarity radius (e.g., 0.8 for cosine similarity >= 0.8)\n",
    "radius = 0.999\n",
    "pairs = count_similar_clip_pairs(radius)\n",
    "\n",
    "for f1, f2, count, avg_dist in sorted(pairs, key=lambda x: -x[2]):\n",
    "    print(f\"{f1} <--> {f2} | similar clips: {count} | avg distance: {avg_dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf210fa",
   "metadata": {},
   "source": [
    "## Evaluate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61aef684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… True Positives (correctly detected): 2\n",
      "âŒ False Positives (wrongly flagged):   0\n",
      "ğŸ” False Negatives (missed duplicates): 4685308\n",
      "ğŸ“ˆ Recall: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "with open(\"dup_groundtruth.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    # if your CSV has a header, skip it:\n",
    "    next(reader, None)\n",
    "    # columns: filename, status\n",
    "    ground_truth = {row[0]: row[1] for row in reader}\n",
    "\n",
    "\n",
    "def evaluate_duplicates(pairs, ground_truth):\n",
    "    def extract_id(filename):\n",
    "        match = re.search(r'TNS_(\\d+)', filename)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    # Normalize detected pairs based on ID number\n",
    "    detected_duplicates = set()\n",
    "    for f1, f2, _, _ in pairs:\n",
    "        id1 = extract_id(f1)\n",
    "        id2 = extract_id(f2)\n",
    "        if id1 and id2:\n",
    "            detected_duplicates.add(tuple(sorted([id1, id2])))\n",
    "\n",
    "    # Normalize ground-truth: find all DUPLICATE â†’ PRINCIPAL mappings\n",
    "    true_duplicates = {k for k, v in ground_truth.items() if v.upper() == \"DUPLICATE\"}\n",
    "    true_pairs = set()\n",
    "    for dup in true_duplicates:\n",
    "        dup_id = extract_id(dup)\n",
    "        for principal, status in ground_truth.items():\n",
    "            if status.upper() == \"PRINCIPAL\":\n",
    "                principal_id = extract_id(principal)\n",
    "                if dup_id and principal_id:\n",
    "                    true_pairs.add(tuple(sorted([dup_id, principal_id])))\n",
    "\n",
    "    # Evaluation\n",
    "    true_positives = detected_duplicates & true_pairs\n",
    "    false_positives = detected_duplicates - true_pairs\n",
    "    false_negatives = true_pairs - detected_duplicates\n",
    "\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if (len(true_positives) + len(false_negatives)) > 0 else 0.0\n",
    "\n",
    "    print(f\"âœ… True Positives (correctly detected): {len(true_positives)}\")\n",
    "    print(f\"âŒ False Positives (wrongly flagged):   {len(false_positives)}\")\n",
    "    print(f\"ğŸ” False Negatives (missed duplicates): {len(false_negatives)}\")\n",
    "    print(f\"ğŸ“ˆ Recall: {recall:.2%}\")\n",
    "\n",
    "    return {\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "eval_result = evaluate_duplicates(pairs, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14c7f8",
   "metadata": {},
   "source": [
    "## Find duplicates and optimal radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85b630e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   radius  true_positives  false_positives  false_negatives        recall\n",
      "0   0.900               3               28          4685307  6.402991e-07\n",
      "1   0.950               2               10          4685308  4.268661e-07\n",
      "2   0.990               2                0          4685308  4.268661e-07\n",
      "3   0.999               2                0          4685308  4.268661e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths to embeddings, lookup, and index\n",
    "EMB_PATH    = 'videoclip_output/video_embeddings.npy'\n",
    "LOOKUP_PATH = 'videoclip_output/embedding_lookup.json'\n",
    "INDEX_PATH  = 'videoclip_output/video_embeddings.index'\n",
    "\n",
    "def count_similar_clip_pairs(radius):\n",
    "    embeddings = np.load(EMB_PATH)\n",
    "    with open(LOOKUP_PATH) as f:\n",
    "        lookup = json.load(f)\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    lims, distances, labels = index.range_search(embeddings, radius)\n",
    "\n",
    "    pair_stats = defaultdict(list)\n",
    "    for q in range(len(embeddings)):\n",
    "        for i in range(lims[q], lims[q+1]):\n",
    "            idx = labels[i]\n",
    "            if idx <= q:\n",
    "                continue\n",
    "            f1 = lookup[str(q)]['video_file']\n",
    "            f2 = lookup[str(idx)]['video_file']\n",
    "            if f1 != f2:\n",
    "                pair = tuple(sorted((f1, f2)))\n",
    "                pair_stats[pair].append(distances[i])\n",
    "\n",
    "    return [\n",
    "        (f1, f2, len(dists), sum(dists)/len(dists))\n",
    "        for (f1, f2), dists in pair_stats.items()\n",
    "    ]\n",
    "\n",
    "def evaluate_duplicates(pairs, ground_truth):\n",
    "    def extract_id(name):\n",
    "        m = re.search(r'TNS_(\\d+)', name)\n",
    "        return m.group(1) if m else None\n",
    "\n",
    "    detected = {\n",
    "        tuple(sorted((extract_id(f1), extract_id(f2))))\n",
    "        for f1, f2, _, _ in pairs\n",
    "    }\n",
    "    truth_dups = {\n",
    "        extract_id(k)\n",
    "        for k, v in ground_truth.items()\n",
    "        if v.upper() == 'DUPLICATE'\n",
    "    }\n",
    "    truth_pairs = {\n",
    "        tuple(sorted((dup, extract_id(pr))))\n",
    "        for dup in truth_dups\n",
    "        for pr, st in ground_truth.items()\n",
    "        if st.upper() == 'PRINCIPAL'\n",
    "    }\n",
    "\n",
    "    tp = len(detected & truth_pairs)\n",
    "    fp = len(detected - truth_pairs)\n",
    "    fn = len(truth_pairs - detected)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# --- Load ground truth ---\n",
    "gt_df = pd.read_csv('dup_groundtruth.csv')  # comma-separated file\n",
    "ground_truth = dict(zip(gt_df['UAR Code'], gt_df['Principal / Duplicate']))\n",
    "\n",
    "# --- Loop over different radii ---\n",
    "radii = [0.9, 0.95, 0.99, 0.999]\n",
    "results = []\n",
    "\n",
    "for r in radii:\n",
    "    pairs   = count_similar_clip_pairs(r)\n",
    "    metrics = evaluate_duplicates(pairs, ground_truth)\n",
    "    results.append({'radius': r, **metrics})\n",
    "\n",
    "# --- Export & display ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('evaluation_results.csv', index=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09882c39",
   "metadata": {},
   "source": [
    "## Search prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LOOKUP_PATH = \"output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"output/video_embeddings.index\"\n",
    "MODEL_CHECKPOINT = \"microsoft/xclip-base-patch32\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Load lookup table and FAISS index\n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "# Load tokenizer & text model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModel.from_pretrained(MODEL_CHECKPOINT).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def search_prompts(prompts, top_k=1):\n",
    "    \"\"\"\n",
    "    Encode text prompts, search the FAISS index, and return\n",
    "    filename + timestamp for each top-k match.\n",
    "    \"\"\"\n",
    "    # Tokenize and encode\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        text_feats = model.get_text_features(**inputs)\n",
    "    text_feats = text_feats.cpu().numpy()\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(text_feats)\n",
    "\n",
    "    # Search\n",
    "    D, I = index.search(text_feats, top_k)\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        matches = []\n",
    "        for score, clip_id in zip(D[i], I[i]):\n",
    "            info = lookup[str(int(clip_id))]\n",
    "            matches.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"file\": info[\"video_file\"],\n",
    "                \"start_time_sec\": info[\"start_time_sec\"],\n",
    "                \"clip_index\": info[\"clip_index\"],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        results.append(matches)\n",
    "    return results\n",
    "\n",
    "prompts = [\n",
    "    \"Videos of a man injured in the daytime. Smoke is rising in the background\",\n",
    "    \"A clown eating a huge bowl of spagetti while riding a bicycle\"\n",
    "]\n",
    "results = search_prompts(prompts, top_k=3)\n",
    "for match_list in results:\n",
    "    for match in match_list:\n",
    "        print(f\"Prompt: {match['prompt']}\")\n",
    "        print(f\"  File: {match['file']}\")\n",
    "        print(f\"  Start time: {match['start_time_sec']}s (clip index {match['clip_index']})\")\n",
    "        print(f\"  Similarity: {match['similarity']:.4f}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dedup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
