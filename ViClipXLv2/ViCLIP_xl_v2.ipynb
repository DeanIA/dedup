{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fd6c74",
   "metadata": {},
   "source": [
    "## Import & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fa6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3492049/4200623658.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import av\n",
    "import cv2\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import faiss\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from modeling import VideoCLIP_XL\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "VIDEO_DIR       = \"../og_ds\"\n",
    "OUTPUT_DIR      = \"ViClipXLv2_output\"\n",
    "FRAMES_PER_CLIP = 8\n",
    "CLIP_DURATION   = 30\n",
    "BATCH_SIZE      = 128\n",
    "\n",
    "# ==== DEVICE SETUP ====\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(f\"device: {device}\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Plain ol' CPU\")\n",
    "\n",
    "# ==== LOAD VideoCLIP-XL-v2 ====\n",
    "model = VideoCLIP_XL()  \n",
    "state_dict = torch.load(\n",
    "    \"./VideoCLIP-XL-v2.bin\",\n",
    "    map_location=device\n",
    ")\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccda9d",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776706a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on batch: IDs 0–127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jupyter-dai7591/.conda/envs/dedup/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on batch: IDs 128–255\n",
      "Running inference on batch: IDs 256–383\n",
      "Running inference on batch: IDs 384–511\n",
      "Running inference on batch: IDs 512–639\n",
      "Running inference on batch: IDs 640–767\n",
      "Running inference on batch: IDs 768–895\n",
      "Running inference on batch: IDs 896–1023\n",
      "Running inference on batch: IDs 1024–1151\n",
      "Running inference on batch: IDs 1152–1279\n",
      "Running inference on batch: IDs 1280–1407\n",
      "Running inference on batch: IDs 1408–1535\n",
      "Running inference on batch: IDs 1536–1663\n",
      "Running inference on batch: IDs 1664–1791\n",
      "Running inference on batch: IDs 1792–1919\n",
      "Running inference on batch: IDs 1920–2047\n",
      "Running inference on batch: IDs 2048–2175\n",
      "Running inference on batch: IDs 2176–2303\n",
      "Running inference on batch: IDs 2304–2431\n",
      "Running inference on batch: IDs 2432–2559\n",
      "Running inference on batch: IDs 2560–2687\n",
      "Running inference on batch: IDs 2688–2815\n",
      "Running inference on batch: IDs 2816–2943\n",
      "Running inference on batch: IDs 2944–3071\n",
      "Running inference on batch: IDs 3072–3199\n",
      "Running inference on batch: IDs 3200–3327\n",
      "Running inference on batch: IDs 3328–3455\n",
      "Running inference on batch: IDs 3456–3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 412\n",
      "Could not find ref with POC 412\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 636\n",
      "Could not find ref with POC 636\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 860\n",
      "Could not find ref with POC 860\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 1084\n",
      "Could not find ref with POC 1084\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1308\n",
      "Could not find ref with POC 1308\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1532\n",
      "Could not find ref with POC 1532\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1788\n",
      "Could not find ref with POC 1788\n",
      "Could not find ref with POC 1884\n",
      "Could not find ref with POC 1884\n",
      "Could not find ref with POC 2012\n",
      "Could not find ref with POC 2012\n",
      "Could not find ref with POC 2108\n",
      "Could not find ref with POC 2108\n",
      "Could not find ref with POC 2236\n",
      "Could not find ref with POC 2236\n",
      "Could not find ref with POC 2332\n",
      "Could not find ref with POC 2332\n",
      "Could not find ref with POC 2460\n",
      "Could not find ref with POC 2460\n",
      "Could not find ref with POC 2556\n",
      "Could not find ref with POC 2556\n",
      "Could not find ref with POC 2684\n",
      "Could not find ref with POC 2684\n",
      "Could not find ref with POC 2780\n",
      "Could not find ref with POC 2780\n",
      "Could not find ref with POC 2908\n",
      "Could not find ref with POC 2908\n",
      "Could not find ref with POC 3004\n",
      "Could not find ref with POC 3004\n",
      "Could not find ref with POC 3132\n",
      "Could not find ref with POC 3132\n",
      "Could not find ref with POC 3228\n",
      "Could not find ref with POC 3228\n",
      "Could not find ref with POC 3356\n",
      "Could not find ref with POC 3356\n",
      "Could not find ref with POC 3452\n",
      "Could not find ref with POC 3452\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 892\n",
      "Could not find ref with POC 892\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 1116\n",
      "Could not find ref with POC 1116\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1340\n",
      "Could not find ref with POC 1340\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1564\n",
      "Could not find ref with POC 1564\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1788\n",
      "Could not find ref with POC 1788\n",
      "Could not find ref with POC 1884\n",
      "Could not find ref with POC 1884\n",
      "Could not find ref with POC 2012\n",
      "Could not find ref with POC 2012\n",
      "Could not find ref with POC 2108\n",
      "Could not find ref with POC 2108\n",
      "Could not find ref with POC 2236\n",
      "Could not find ref with POC 2236\n",
      "Could not find ref with POC 2332\n",
      "Could not find ref with POC 2332\n",
      "Could not find ref with POC 2460\n",
      "Could not find ref with POC 2460\n",
      "Could not find ref with POC 2556\n",
      "Could not find ref with POC 2556\n",
      "Could not find ref with POC 2684\n",
      "Could not find ref with POC 2684\n",
      "Could not find ref with POC 2780\n",
      "Could not find ref with POC 2780\n",
      "Could not find ref with POC 2908\n",
      "Could not find ref with POC 2908\n",
      "Could not find ref with POC 3004\n",
      "Could not find ref with POC 3004\n",
      "Could not find ref with POC 3132\n",
      "Could not find ref with POC 3132\n",
      "Could not find ref with POC 3228\n",
      "Could not find ref with POC 3228\n",
      "Could not find ref with POC 3356\n",
      "Could not find ref with POC 3356\n",
      "Could not find ref with POC 3452\n",
      "Could not find ref with POC 3452\n",
      "Could not find ref with POC 3580\n",
      "Could not find ref with POC 3580\n",
      "Could not find ref with POC 3676\n",
      "Could not find ref with POC 3676\n",
      "Could not find ref with POC 3804\n",
      "Could not find ref with POC 3804\n",
      "Could not find ref with POC 3900\n",
      "Could not find ref with POC 3900\n",
      "Could not find ref with POC 4028\n",
      "Could not find ref with POC 4028\n",
      "Could not find ref with POC 4156\n",
      "Could not find ref with POC 4156\n",
      "Could not find ref with POC 4252\n",
      "Could not find ref with POC 4252\n",
      "Could not find ref with POC 4380\n",
      "Could not find ref with POC 4380\n",
      "Could not find ref with POC 4476\n",
      "Could not find ref with POC 4476\n",
      "Could not find ref with POC 4604\n",
      "Could not find ref with POC 4604\n",
      "Could not find ref with POC 4700\n",
      "Could not find ref with POC 4700\n",
      "Could not find ref with POC 4828\n",
      "Could not find ref with POC 4828\n",
      "Could not find ref with POC 4924\n",
      "Could not find ref with POC 4924\n",
      "Could not find ref with POC 5052\n",
      "Could not find ref with POC 5052\n",
      "Could not find ref with POC 5148\n",
      "Could not find ref with POC 5148\n",
      "Could not find ref with POC 5276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on batch: IDs 3584–3711\n",
      "Running inference on batch: IDs 3712–3839\n",
      "Running inference on batch: IDs 3840–3967\n",
      "Running inference on batch: IDs 3968–4095\n",
      "Running inference on batch: IDs 4096–4223\n",
      "Running inference on batch: IDs 4224–4351\n",
      "Running inference on batch: IDs 4352–4479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find ref with POC 5276\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 412\n",
      "Could not find ref with POC 412\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 892\n",
      "Could not find ref with POC 892\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 1116\n",
      "Could not find ref with POC 1116\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1340\n",
      "Could not find ref with POC 1340\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1564\n",
      "Could not find ref with POC 1564\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 412\n",
      "Could not find ref with POC 412\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on batch: IDs 4480–4607\n",
      "Running inference on batch: IDs 4608–4735\n",
      "Running inference on batch: IDs 4736–4863\n",
      "Running inference on batch: IDs 4864–4991\n",
      "Running inference on batch: IDs 4992–5119\n",
      "Running inference on batch: IDs 5120–5247\n",
      "Running inference on batch: IDs 5248–5375\n",
      "Running inference on batch: IDs 5376–5503\n",
      "Running inference on batch: IDs 5504–5631\n",
      "Running inference on batch: IDs 5632–5759\n",
      "Running inference on batch: IDs 5760–5887\n",
      "Running inference on batch: IDs 5888–6015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 412\n",
      "Could not find ref with POC 412\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 636\n",
      "Could not find ref with POC 636\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 860\n",
      "Could not find ref with POC 860\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 1084\n",
      "Could not find ref with POC 1084\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1308\n",
      "Could not find ref with POC 1308\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1532\n",
      "Could not find ref with POC 1532\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1788\n",
      "Could not find ref with POC 1788\n",
      "Could not find ref with POC 1884\n",
      "Could not find ref with POC 1884\n",
      "Could not find ref with POC 2012\n",
      "Could not find ref with POC 2012\n",
      "Could not find ref with POC 2108\n",
      "Could not find ref with POC 2108\n",
      "Could not find ref with POC 2236\n",
      "Could not find ref with POC 2236\n",
      "Could not find ref with POC 2332\n",
      "Could not find ref with POC 2332\n",
      "Could not find ref with POC 2460\n",
      "Could not find ref with POC 2460\n",
      "Could not find ref with POC 2556\n",
      "Could not find ref with POC 2556\n",
      "Could not find ref with POC 2684\n",
      "Could not find ref with POC 2684\n",
      "Could not find ref with POC 2780\n",
      "Could not find ref with POC 2780\n",
      "Could not find ref with POC 2908\n",
      "Could not find ref with POC 2908\n",
      "Could not find ref with POC 3004\n",
      "Could not find ref with POC 3004\n",
      "Could not find ref with POC 3132\n",
      "Could not find ref with POC 3132\n",
      "Could not find ref with POC 3228\n",
      "Could not find ref with POC 3228\n",
      "Could not find ref with POC 3356\n",
      "Could not find ref with POC 3356\n",
      "Could not find ref with POC 3452\n",
      "Could not find ref with POC 3452\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 92\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 188\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 316\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 444\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 540\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 668\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 764\n",
      "Could not find ref with POC 892\n",
      "Could not find ref with POC 892\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 988\n",
      "Could not find ref with POC 1116\n",
      "Could not find ref with POC 1116\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1212\n",
      "Could not find ref with POC 1340\n",
      "Could not find ref with POC 1340\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1436\n",
      "Could not find ref with POC 1564\n",
      "Could not find ref with POC 1564\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1660\n",
      "Could not find ref with POC 1788\n",
      "Could not find ref with POC 1788\n",
      "Could not find ref with POC 1884\n",
      "Could not find ref with POC 1884\n",
      "Could not find ref with POC 2012\n",
      "Could not find ref with POC 2012\n",
      "Could not find ref with POC 2108\n",
      "Could not find ref with POC 2108\n",
      "Could not find ref with POC 2236\n",
      "Could not find ref with POC 2236\n",
      "Could not find ref with POC 2332\n",
      "Could not find ref with POC 2332\n",
      "Could not find ref with POC 2460\n",
      "Could not find ref with POC 2460\n",
      "Could not find ref with POC 2556\n",
      "Could not find ref with POC 2556\n",
      "Could not find ref with POC 2684\n",
      "Could not find ref with POC 2684\n",
      "Could not find ref with POC 2780\n",
      "Could not find ref with POC 2780\n",
      "Could not find ref with POC 2908\n",
      "Could not find ref with POC 2908\n",
      "Could not find ref with POC 3004\n",
      "Could not find ref with POC 3004\n",
      "Could not find ref with POC 3132\n",
      "Could not find ref with POC 3132\n",
      "Could not find ref with POC 3228\n",
      "Could not find ref with POC 3228\n",
      "Could not find ref with POC 3356\n",
      "Could not find ref with POC 3356\n",
      "Could not find ref with POC 3452\n",
      "Could not find ref with POC 3452\n",
      "Could not find ref with POC 3580\n",
      "Could not find ref with POC 3580\n",
      "Could not find ref with POC 3676\n",
      "Could not find ref with POC 3676\n",
      "Could not find ref with POC 3804\n",
      "Could not find ref with POC 3804\n",
      "Could not find ref with POC 3900\n",
      "Could not find ref with POC 3900\n",
      "Could not find ref with POC 4028\n",
      "Could not find ref with POC 4028\n",
      "Could not find ref with POC 4156\n",
      "Could not find ref with POC 4156\n",
      "Could not find ref with POC 4252\n",
      "Could not find ref with POC 4252\n",
      "Could not find ref with POC 4380\n",
      "Could not find ref with POC 4380\n",
      "Could not find ref with POC 4476\n",
      "Could not find ref with POC 4476\n",
      "Could not find ref with POC 4604\n",
      "Could not find ref with POC 4604\n",
      "Could not find ref with POC 4700\n",
      "Could not find ref with POC 4700\n",
      "Could not find ref with POC 4828\n",
      "Could not find ref with POC 4828\n",
      "Could not find ref with POC 4924\n",
      "Could not find ref with POC 4924\n",
      "Could not find ref with POC 5052\n",
      "Could not find ref with POC 5052\n",
      "Could not find ref with POC 5148\n",
      "Could not find ref with POC 5148\n",
      "Could not find ref with POC 5276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on batch: IDs 6016–6143\n",
      "Running inference on batch: IDs 6144–6271\n",
      "Running inference on batch: IDs 6272–6399\n",
      "Running inference on batch: IDs 6400–6527\n",
      "Running inference on batch: IDs 6528–6655\n",
      "Running inference on batch: IDs 6656–6783\n",
      "Running inference on batch: IDs 6784–6911\n",
      "Running inference on batch: IDs 6912–7039\n",
      "Running inference on batch: IDs 7040–7167\n",
      "Running inference on batch: IDs 7168–7295\n",
      "Running inference on batch: IDs 7296–7423\n",
      "Running inference on batch: IDs 7424–7551\n",
      "Running inference on batch: IDs 7552–7679\n",
      "Running inference on batch: IDs 7680–7807\n"
     ]
    }
   ],
   "source": [
    "# ==== FRAME EXTRACTOR ====\n",
    "def read_video_clip(container, indices, fps):\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        ts = int((idx / fps) * 1e6)\n",
    "        container.seek(ts, any_frame=False, backward=True)\n",
    "        for frame in container.decode(video=0):\n",
    "            frames.append(frame.to_ndarray(format=\"rgb24\"))\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "# ==== FRAME NORMALIZATION + TENSOR CONVERSION ====\n",
    "v_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
    "v_std  = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n",
    "\n",
    "def normalize(data):\n",
    "    return (data / 255.0 - v_mean) / v_std\n",
    "\n",
    "def frames2tensor(vid_list, fnum=FRAMES_PER_CLIP, target_size=(224, 224), device=device):\n",
    "    assert len(vid_list) >= fnum\n",
    "    step = len(vid_list) // fnum\n",
    "    sampled = vid_list[::step][:fnum]\n",
    "    resized = [cv2.resize(frame[:, :, ::-1], target_size) for frame in sampled]\n",
    "    normalized = [np.expand_dims(normalize(frame), axis=0) for frame in resized]  # (1, H, W, C)\n",
    "    tube = np.stack(normalized, axis=1)  # (1, T, H, W, C)\n",
    "    tube = np.transpose(tube, (0, 1, 4, 2, 3))  # (1, T, C, H, W)\n",
    "    return torch.from_numpy(tube).to(device, non_blocking=True).float()\n",
    "\n",
    "# ==== BATCH PROCESSING FUNCTION ====\n",
    "def process_directory_to_embeddings(\n",
    "    video_dir: str,\n",
    "    clip_len: int = FRAMES_PER_CLIP,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    OUTPUT_DIR: str = OUTPUT_DIR,\n",
    "):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    embedding_batches = []\n",
    "    lookup_dict = {}\n",
    "    global_clip_id = 0\n",
    "    current_batch = []\n",
    "    current_meta  = []\n",
    "\n",
    "    exts = ('.mp4', '.mov', '.avi', '.webm', '.mkv', '.flv', '.wmv')\n",
    "\n",
    "    for filename in sorted(os.listdir(video_dir)):\n",
    "        if not filename.lower().endswith(exts):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_dir, filename)\n",
    "        container  = av.open(video_path)\n",
    "        stream     = container.streams.video[0]\n",
    "        total_frames = stream.frames\n",
    "        fps = float(stream.average_rate) if stream.average_rate else 1.0\n",
    "\n",
    "        clip_index = 0\n",
    "        window_size = int(CLIP_DURATION * fps)\n",
    "\n",
    "        for start in range(0, total_frames - window_size + 1, window_size):\n",
    "            indices = np.linspace(start, start + window_size, num=clip_len, endpoint=False, dtype=np.int64)\n",
    "            frames  = read_video_clip(container, indices, fps)\n",
    "            if len(frames) < clip_len:\n",
    "                continue\n",
    "\n",
    "            current_batch.append(frames)\n",
    "            current_meta.append({\n",
    "                \"clip_id\":        global_clip_id,\n",
    "                \"clip_index\":     clip_index,\n",
    "                \"video_file\":     filename,\n",
    "                \"start_time_sec\": int(start / fps),\n",
    "            })\n",
    "\n",
    "            clip_index += 1\n",
    "            global_clip_id += 1\n",
    "\n",
    "            if len(current_batch) == batch_size:\n",
    "                print(f\"Running inference on batch: IDs {current_meta[0]['clip_id']}–{current_meta[-1]['clip_id']}\")\n",
    "                batch_tensor = torch.cat([\n",
    "                    frames2tensor(frames, fnum=clip_len, device=device)\n",
    "                    for frames in current_batch\n",
    "                ], dim=0)\n",
    "                with torch.no_grad():\n",
    "                    embeds = model.vision_model.get_vid_features(batch_tensor).cpu().numpy()\n",
    "                embedding_batches.append(embeds)\n",
    "                for m in current_meta:\n",
    "                    lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "                current_batch = []\n",
    "                current_meta  = []\n",
    "\n",
    "        container.close()\n",
    "\n",
    "    # Final batch\n",
    "    if current_batch:\n",
    "        print(f\"Running inference on final batch: IDs {current_meta[0]['clip_id']}–{current_meta[-1]['clip_id']}\")\n",
    "        batch_tensor = torch.cat([\n",
    "            frames2tensor(frames, fnum=clip_len, device=device)\n",
    "            for frames in current_batch\n",
    "        ], dim=0)\n",
    "        with torch.no_grad():\n",
    "            embeds = model.vision_model.get_vid_features(batch_tensor).cpu().numpy()\n",
    "        embedding_batches.append(embeds)\n",
    "        for m in current_meta:\n",
    "            lookup_dict[str(m[\"clip_id\"])] = m\n",
    "\n",
    "    # Save outputs\n",
    "    all_embeddings = np.vstack(embedding_batches)\n",
    "    np.save(os.path.join(OUTPUT_DIR, \"video_embeddings.npy\"), all_embeddings)\n",
    "    with open(os.path.join(OUTPUT_DIR, \"embedding_lookup.json\"), \"w\") as f:\n",
    "        json.dump(lookup_dict, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Saved embeddings to {OUTPUT_DIR}/video_embeddings.npy\")\n",
    "    print(f\"✅ Saved lookup to    {OUTPUT_DIR}/embedding_lookup.json\")\n",
    "\n",
    "# ==== MAIN ENTRY ====\n",
    "if __name__ == \"__main__\":\n",
    "    process_directory_to_embeddings(VIDEO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5094e",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54de12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Run:\n",
      "  tensorboard --logdir=runs/embeds\n",
      "Then open http://localhost:6006/#projector\n"
     ]
    }
   ],
   "source": [
    "# Load lookup metadata\n",
    "with open(\"ViClipXLv2_output/embedding_lookup.json\", \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Ensure entries are sorted by clip_id (assumed to match row index)\n",
    "sorted_lookup = sorted(lookup.values(), key=lambda x: int(x[\"clip_id\"]))\n",
    "\n",
    "# Create metadata strings like: \"video.mp4[3] @ 90s\"\n",
    "metadata = [\n",
    "    f'{entry[\"video_file\"]}[{entry[\"clip_index\"]}] @ {entry[\"start_time_sec\"]}s'\n",
    "    for entry in sorted_lookup\n",
    "]\n",
    "\n",
    "# Load embedding matrix\n",
    "emb_matrix = np.load(\"ViClipXLv2_output/video_embeddings.npy\")\n",
    "\n",
    "# Ensure alignment\n",
    "assert emb_matrix.shape[0] == len(metadata), (\n",
    "    f\"❌ {emb_matrix.shape[0]} embeddings vs {len(metadata)} metadata entries\"\n",
    ")\n",
    "\n",
    "# Write to TensorBoard\n",
    "writer = SummaryWriter(log_dir=\"runs/embeds\")\n",
    "writer.add_embedding(\n",
    "    emb_matrix,\n",
    "    metadata=metadata,\n",
    "    tag=\"my_embeddings\"\n",
    ")\n",
    "writer.close()\n",
    "\n",
    "print(\"✅ Done. Run:\\n  tensorboard --logdir=runs/embeds\\nThen open http://localhost:6006/#projector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c43c8",
   "metadata": {},
   "source": [
    "## Index in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28beaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_PATH    = \"ViClipXLv2_output/video_embeddings.npy\"\n",
    "LOOKUP_PATH = \"ViClipXLv2_output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"ViClipXLv2_output/video_embeddings.index\"\n",
    "\n",
    "# Load embeddings and lookup dict\n",
    "embeddings = np.load(EMB_PATH)  \n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "\n",
    "# Build FAISS index (inner-product) and add IDs\n",
    "faiss.normalize_L2(embeddings)\n",
    "dim   = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "ids = np.arange(embeddings.shape[0], dtype=\"int64\")\n",
    "index.add_with_ids(embeddings, ids)\n",
    "\n",
    "# Save the index for later\n",
    "faiss.write_index(index, INDEX_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4f644",
   "metadata": {},
   "source": [
    "## Find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e27c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNS_0030_V.mp4 <--> TNS_0031_V.mp4 | similar clips: 68 | avg distance: 1.0000\n",
      "TNS_0024_V.mp4 <--> TNS_0025_V.mp4 | similar clips: 19 | avg distance: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "def count_similar_clip_pairs(radius: float):\n",
    "    \"\"\"\n",
    "    For each unique pair of video files, count how many clip pairs are similar\n",
    "    (i.e., within the given radius), and record the average distance.\n",
    "    Returns: list of (file1, file2, count, avg_distance)\n",
    "    \"\"\"\n",
    "    embeddings = np.load(EMB_PATH)\n",
    "    with open(LOOKUP_PATH, \"r\") as f:\n",
    "        lookup = json.load(f)\n",
    "\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    lims, distances, labels = index.range_search(embeddings, radius)\n",
    "\n",
    "    # Map (file1, file2) to list of distances\n",
    "    pair_stats = defaultdict(list)\n",
    "\n",
    "    for q in range(len(embeddings)):\n",
    "        start, end = lims[q], lims[q+1]\n",
    "        for i in range(start, end):\n",
    "            idx = labels[i]\n",
    "            if idx <= q:\n",
    "                continue  # skip self or repeated\n",
    "            f1 = lookup[str(q)][\"video_file\"]\n",
    "            f2 = lookup[str(idx)][\"video_file\"]\n",
    "            if f1 != f2:\n",
    "                key = tuple(sorted((f1, f2)))\n",
    "                pair_stats[key].append(distances[i])\n",
    "\n",
    "    results = []\n",
    "    for (f1, f2), dists in pair_stats.items():\n",
    "        avg_dist = sum(dists) / len(dists)\n",
    "        results.append((f1, f2, len(dists), avg_dist))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Choose your similarity radius (e.g., 0.8 for cosine similarity >= 0.8)\n",
    "radius = 0.999\n",
    "pairs = count_similar_clip_pairs(radius)\n",
    "\n",
    "for f1, f2, count, avg_dist in sorted(pairs, key=lambda x: -x[2]):\n",
    "    print(f\"{f1} <--> {f2} | similar clips: {count} | avg distance: {avg_dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf210fa",
   "metadata": {},
   "source": [
    "## Evaluate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aef684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ True Positives (correctly detected): 2\n",
      "❌ False Positives (wrongly flagged):   0\n",
      "🔍 False Negatives (missed duplicates): 4685308\n",
      "📈 Recall: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "import faiss\n",
    "import networkx as nx\n",
    "\n",
    "# === Configuration ===\n",
    "VIDEO_DIR         = \"../videos\"\n",
    "GROUNDTRUTH_CSV   = \"../dup_groundtruth.csv\"\n",
    "LOOKUP_JSON       = \"embedding_lookup.json\"\n",
    "EMB_PATH          = \"embeddings.npy\"\n",
    "INDEX_PATH        = \"index.index\"\n",
    "RADII             = [0.92, 0.94, 0.96, 0.98, 0.990, 0.995, 0.999]\n",
    "OUTPUT_CSV        = \"radius_results.csv\"\n",
    "\n",
    "# === 1. Compute sharpness and duration metrics ===\n",
    "def compute_video_quality_metrics(video_dir):\n",
    "    video_variance = {}\n",
    "    video_duration = {}\n",
    "    for fn in os.listdir(video_dir):\n",
    "        if not fn.lower().endswith(('.mp4', '.mov', '.avi', '.webm', '.mkv', '.flv', '.wmv')):\n",
    "            continue\n",
    "        path = os.path.join(video_dir, fn)\n",
    "        try:\n",
    "            container = av.open(path)\n",
    "            vs = container.streams.video[0]\n",
    "            vs.codec_context.skip_frame = \"NONKEY\"\n",
    "            frame = next(container.decode(vs), None)\n",
    "            if frame is None:\n",
    "                raise RuntimeError(\"no frame decoded\")\n",
    "            img = frame.to_ndarray(format=\"bgr24\")\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            lap = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "            video_variance[fn] = float(lap.var())\n",
    "            duration = (vs.duration * vs.time_base) if vs.duration and vs.time_base else 0.0\n",
    "            video_duration[fn] = float(duration)\n",
    "            container.close()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {fn} failed: {e}\")\n",
    "            video_variance[fn] = None\n",
    "            video_duration[fn] = 0.0\n",
    "    return video_variance, video_duration\n",
    "\n",
    "video_variance, video_duration = compute_video_quality_metrics(VIDEO_DIR)\n",
    "\n",
    "# === 2. Load ground truth ===\n",
    "with open(GROUNDTRUTH_CSV, newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    ground_truth = {row[0]: row[1] for row in reader}\n",
    "\n",
    "# === 3. Load lookup, embeddings, and index ===\n",
    "with open(LOOKUP_JSON) as f:\n",
    "    lookup = json.load(f)\n",
    "embeddings = np.load(EMB_PATH)\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# === 4. Utility functions ===\n",
    "def extract_id(fn):\n",
    "    m = re.search(r'TNS_(\\d+)', fn)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def build_clusters_from_faiss(labels, lims):\n",
    "    G = nx.Graph()\n",
    "    for q in range(len(lims) - 1):\n",
    "        for i in range(lims[q], lims[q + 1]):\n",
    "            k = labels[i]\n",
    "            if k != q:\n",
    "                G.add_edge(q, k)\n",
    "    return list(nx.connected_components(G))\n",
    "\n",
    "def assign_roles_by_heuristics(cluster):\n",
    "    def quality_key(cid):\n",
    "        fn = lookup[str(cid)][\"video_file\"]\n",
    "        var = video_variance.get(fn) or 0.0\n",
    "        dur = video_duration.get(fn) or 0.0\n",
    "        return (var, dur, fn)\n",
    "    best = max(cluster, key=quality_key)\n",
    "    principal = lookup[str(best)][\"video_file\"]\n",
    "    duplicates = [lookup[str(c)][\"video_file\"] for c in cluster if c != best]\n",
    "    return principal, duplicates\n",
    "\n",
    "def evaluate_clusters(clusters):\n",
    "    gt_principals = {fn for fn, r in ground_truth.items() if r.upper() == \"PRINCIPAL\"}\n",
    "    gt_duplicates = {fn for fn, r in ground_truth.items() if r.upper() == \"DUPLICATE\"}\n",
    "    tp, fp, fn_set = set(), set(), set()\n",
    "\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) <= 1:\n",
    "            continue\n",
    "        p_fn, d_list = assign_roles_by_heuristics(cluster)\n",
    "        for d_fn in d_list:\n",
    "            pid, did = extract_id(p_fn), extract_id(d_fn)\n",
    "            pair = (pid, did)\n",
    "            if d_fn in gt_duplicates and p_fn in gt_principals:\n",
    "                tp.add(pair)\n",
    "            else:\n",
    "                fp.add(pair)\n",
    "\n",
    "    for d_fn in gt_duplicates:\n",
    "        for p_fn in gt_principals:\n",
    "            pid, did = extract_id(p_fn), extract_id(d_fn)\n",
    "            if pid and did and (pid, did) not in tp:\n",
    "                fn_set.add((pid, did))\n",
    "\n",
    "    recall = len(tp) / (len(tp) + len(fn_set)) if (len(tp) + len(fn_set)) > 0 else 0.0\n",
    "    return {\"tp\": len(tp), \"fp\": len(fp), \"fn\": len(fn_set), \"recall\": recall}\n",
    "\n",
    "# === 5. Sweep radii ===\n",
    "results = {}\n",
    "for r in RADII:\n",
    "    lims, distances, labels = index.range_search(embeddings, r)\n",
    "    clusters = build_clusters_from_faiss(labels, lims)\n",
    "    res = evaluate_clusters(clusters)\n",
    "    results[r] = res\n",
    "    print(f\"--- Radius {r:.3f} → TP:{res['tp']} FP:{res['fp']} FN:{res['fn']} Recall:{res['recall']:.2%}\")\n",
    "\n",
    "# === 6. Summary ===\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Radius |  TP   FP   FN   Recall\")\n",
    "for r, m in results.items():\n",
    "    print(f\"{r:6.3f} | {m['tp']:4d} {m['fp']:4d} {m['fn']:4d}  {m['recall']:.1%}\")\n",
    "\n",
    "# === 7. Export results to CSV ===\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"radius\", \"tp\", \"fp\", \"fn\", \"recall\"])\n",
    "    for r in sorted(results):\n",
    "        m = results[r]\n",
    "        writer.writerow([r, m[\"tp\"], m[\"fp\"], m[\"fn\"], f\"{m['recall']:.4f}\"])\n",
    "\n",
    "print(f\"\\nResults exported to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14c7f8",
   "metadata": {},
   "source": [
    "## Find duplicates and optimal radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b630e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   radius  true_positives  false_positives  false_negatives        recall\n",
      "0   0.900               6               23          4685304  1.280598e-06\n",
      "1   0.950               2                8          4685308  4.268661e-07\n",
      "2   0.990               2                0          4685308  4.268661e-07\n",
      "3   0.999               2                0          4685308  4.268661e-07\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "import faiss\n",
    "import networkx as nx\n",
    "\n",
    "# === Configuration ===\n",
    "VIDEO_DIR         = \"../videos\"\n",
    "GROUNDTRUTH_CSV   = \"../dup_groundtruth.csv\"\n",
    "LOOKUP_JSON       = \"embedding_lookup.json\"\n",
    "EMB_PATH          = \"embeddings.npy\"\n",
    "INDEX_PATH        = \"index.index\"\n",
    "RADII             = [0.92, 0.94, 0.96, 0.98, 0.990, 0.995, 0.999]\n",
    "OUTPUT_CSV        = \"radius_results.csv\"\n",
    "\n",
    "# === 1. Compute sharpness and duration metrics ===\n",
    "def compute_video_quality_metrics(video_dir):\n",
    "    video_variance = {}\n",
    "    video_duration = {}\n",
    "    for fn in os.listdir(video_dir):\n",
    "        if not fn.lower().endswith(('.mp4', '.mov', '.avi', '.webm', '.mkv', '.flv', '.wmv')):\n",
    "            continue\n",
    "        path = os.path.join(video_dir, fn)\n",
    "        try:\n",
    "            container = av.open(path)\n",
    "            vs = container.streams.video[0]\n",
    "            vs.codec_context.skip_frame = \"NONKEY\"\n",
    "            frame = next(container.decode(vs), None)\n",
    "            if frame is None:\n",
    "                raise RuntimeError(\"no frame decoded\")\n",
    "            img = frame.to_ndarray(format=\"bgr24\")\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            lap = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "            video_variance[fn] = float(lap.var())\n",
    "            duration = (vs.duration * vs.time_base) if vs.duration and vs.time_base else 0.0\n",
    "            video_duration[fn] = float(duration)\n",
    "            container.close()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {fn} failed: {e}\")\n",
    "            video_variance[fn] = None\n",
    "            video_duration[fn] = 0.0\n",
    "    return video_variance, video_duration\n",
    "\n",
    "video_variance, video_duration = compute_video_quality_metrics(VIDEO_DIR)\n",
    "\n",
    "# === 2. Load ground truth ===\n",
    "with open(GROUNDTRUTH_CSV, newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)\n",
    "    ground_truth = {row[0]: row[1] for row in reader}\n",
    "\n",
    "# === 3. Load lookup, embeddings, and index ===\n",
    "with open(LOOKUP_JSON) as f:\n",
    "    lookup = json.load(f)\n",
    "embeddings = np.load(EMB_PATH)\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "# === 4. Utility functions ===\n",
    "def extract_id(fn):\n",
    "    m = re.search(r'TNS_(\\d+)', fn)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def build_clusters_from_faiss(labels, lims):\n",
    "    G = nx.Graph()\n",
    "    for q in range(len(lims) - 1):\n",
    "        for i in range(lims[q], lims[q + 1]):\n",
    "            k = labels[i]\n",
    "            if k != q:\n",
    "                G.add_edge(q, k)\n",
    "    return list(nx.connected_components(G))\n",
    "\n",
    "def assign_roles_by_heuristics(cluster):\n",
    "    def quality_key(cid):\n",
    "        fn = lookup[str(cid)][\"video_file\"]\n",
    "        var = video_variance.get(fn) or 0.0\n",
    "        dur = video_duration.get(fn) or 0.0\n",
    "        return (var, dur, fn)\n",
    "    best = max(cluster, key=quality_key)\n",
    "    principal = lookup[str(best)][\"video_file\"]\n",
    "    duplicates = [lookup[str(c)][\"video_file\"] for c in cluster if c != best]\n",
    "    return principal, duplicates\n",
    "\n",
    "def evaluate_clusters(clusters):\n",
    "    gt_principals = {fn for fn, r in ground_truth.items() if r.upper() == \"PRINCIPAL\"}\n",
    "    gt_duplicates = {fn for fn, r in ground_truth.items() if r.upper() == \"DUPLICATE\"}\n",
    "    tp, fp, fn_set = set(), set(), set()\n",
    "\n",
    "    for cluster in clusters:\n",
    "        if len(cluster) <= 1:\n",
    "            continue\n",
    "        p_fn, d_list = assign_roles_by_heuristics(cluster)\n",
    "        for d_fn in d_list:\n",
    "            pid, did = extract_id(p_fn), extract_id(d_fn)\n",
    "            pair = (pid, did)\n",
    "            if d_fn in gt_duplicates and p_fn in gt_principals:\n",
    "                tp.add(pair)\n",
    "            else:\n",
    "                fp.add(pair)\n",
    "\n",
    "    for d_fn in gt_duplicates:\n",
    "        for p_fn in gt_principals:\n",
    "            pid, did = extract_id(p_fn), extract_id(d_fn)\n",
    "            if pid and did and (pid, did) not in tp:\n",
    "                fn_set.add((pid, did))\n",
    "\n",
    "    recall = len(tp) / (len(tp) + len(fn_set)) if (len(tp) + len(fn_set)) > 0 else 0.0\n",
    "    return {\"tp\": len(tp), \"fp\": len(fp), \"fn\": len(fn_set), \"recall\": recall}\n",
    "\n",
    "# === 5. Sweep radii ===\n",
    "results = {}\n",
    "for r in RADII:\n",
    "    lims, distances, labels = index.range_search(embeddings, r)\n",
    "    clusters = build_clusters_from_faiss(labels, lims)\n",
    "    res = evaluate_clusters(clusters)\n",
    "    results[r] = res\n",
    "    print(f\"--- Radius {r:.3f} → TP:{res['tp']} FP:{res['fp']} FN:{res['fn']} Recall:{res['recall']:.2%}\")\n",
    "\n",
    "# === 6. Summary ===\n",
    "print(\"\\nSummary:\")\n",
    "print(\"Radius |  TP   FP   FN   Recall\")\n",
    "for r, m in results.items():\n",
    "    print(f\"{r:6.3f} | {m['tp']:4d} {m['fp']:4d} {m['fn']:4d}  {m['recall']:.1%}\")\n",
    "\n",
    "# === 7. Export results to CSV ===\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"radius\", \"tp\", \"fp\", \"fn\", \"recall\"])\n",
    "    for r in sorted(results):\n",
    "        m = results[r]\n",
    "        writer.writerow([r, m[\"tp\"], m[\"fp\"], m[\"fn\"], f\"{m['recall']:.4f}\"])\n",
    "\n",
    "print(f\"\\nResults exported to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09882c39",
   "metadata": {},
   "source": [
    "## Search prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc9941",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/embedding_lookup.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ────────────────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load lookup table and FAISS index\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLOOKUP_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     16\u001b[0m     lookup \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     17\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mread_index(INDEX_PATH)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/embedding_lookup.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "LOOKUP_PATH = \"output/embedding_lookup.json\"\n",
    "INDEX_PATH  = \"output/video_embeddings.index\"\n",
    "MODEL_CHECKPOINT = \"microsoft/xclip-base-patch32\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Load lookup table and FAISS index\n",
    "with open(LOOKUP_PATH, \"r\") as f:\n",
    "    lookup = json.load(f)\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "# Load tokenizer & text model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = AutoModel.from_pretrained(MODEL_CHECKPOINT).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def search_prompts(prompts, top_k=1):\n",
    "    \"\"\"\n",
    "    Encode text prompts, search the FAISS index, and return\n",
    "    filename + timestamp for each top-k match.\n",
    "    \"\"\"\n",
    "    # Tokenize and encode\n",
    "    inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        text_feats = model.get_text_features(**inputs)\n",
    "    text_feats = text_feats.cpu().numpy()\n",
    "\n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(text_feats)\n",
    "\n",
    "    # Search\n",
    "    D, I = index.search(text_feats, top_k)\n",
    "\n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        matches = []\n",
    "        for score, clip_id in zip(D[i], I[i]):\n",
    "            info = lookup[str(int(clip_id))]\n",
    "            matches.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"file\": info[\"video_file\"],\n",
    "                \"start_time_sec\": info[\"start_time_sec\"],\n",
    "                \"clip_index\": info[\"clip_index\"],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        results.append(matches)\n",
    "    return results\n",
    "\n",
    "prompts = [\n",
    "    \"Videos of a man injured in the daytime. Smoke is rising in the background\",\n",
    "    \"A clown eating a huge bowl of spagetti while riding a bicycle\"\n",
    "]\n",
    "results = search_prompts(prompts, top_k=3)\n",
    "for match_list in results:\n",
    "    for match in match_list:\n",
    "        print(f\"Prompt: {match['prompt']}\")\n",
    "        print(f\"  File: {match['file']}\")\n",
    "        print(f\"  Start time: {match['start_time_sec']}s (clip index {match['clip_index']})\")\n",
    "        print(f\"  Similarity: {match['similarity']:.4f}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dedup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
